"""ì‹œìŠ¤í…œ ìƒíƒœ API ì—”ë“œí¬ì¸íŠ¸"""
from fastapi import APIRouter, Depends, Query, HTTPException, BackgroundTasks
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, text
from datetime import datetime
from typing import Dict, Any, List
from collections import Counter
from pathlib import Path

from app.database import get_db
from app.models.huggingface import HuggingFaceModel
from app.models.github import GitHubProject
from app.models.youtube import YouTubeVideo
from app.models.paper import AIPaper
from app.models.news import AINews
from app.models.conference import AIConference
from app.models.ai_tool import AITool
from app.models.job_trend import AIJobTrend
from app.models.policy import AIPolicy
from app.services.scheduler import collect_all_data, scheduler, get_scheduler_runtime_status
from app.config import get_settings
from app.cache import cache_get, cache_set, TTL_SYSTEM_STATUS, TTL_KEYWORDS, get_redis
import asyncio

router = APIRouter()
settings = get_settings()


@router.get("/status")
async def get_system_status(db: AsyncSession = Depends(get_db)) -> Dict[str, Any]:
    """
    ì‹œìŠ¤í…œ ì „ì²´ ìƒíƒœ ì¡°íšŒ

    - ë°±ì—”ë“œ ì„œë²„ ìƒíƒœ
    - ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ
    - ê° ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ê°œìˆ˜ ë° ìµœì‹  ì—…ë°ì´íŠ¸ ì‹œê°„
    """
    cache_key = "system:status"
    cached = await cache_get(cache_key)
    if cached is not None:
        return cached

    # Database connectivity test
    db_connected = False
    try:
        await db.execute(text("SELECT 1"))
        db_connected = True
    except Exception:
        pass

    # Get counts and latest updates for each category
    categories_status = {}

    # Hugging Face Models
    try:
        hf_count = await db.execute(select(func.count()).select_from(HuggingFaceModel))
        hf_total = hf_count.scalar()

        hf_latest = await db.execute(
            select(HuggingFaceModel.collected_at)
            .order_by(HuggingFaceModel.collected_at.desc())
            .limit(1)
        )
        hf_last_update = hf_latest.scalar_one_or_none()

        categories_status["huggingface"] = {
            "name": "Hugging Face ëª¨ë¸",
            "icon": "ğŸ¤—",
            "total": hf_total,
            "last_update": hf_last_update.isoformat() if hf_last_update else None,
            "status": "healthy" if hf_total > 0 else "no_data"
        }
    except Exception as e:
        categories_status["huggingface"] = {
            "name": "Hugging Face ëª¨ë¸",
            "icon": "ğŸ¤—",
            "total": 0,
            "last_update": None,
            "status": "error",
            "error": str(e)
        }

    # GitHub Projects
    try:
        gh_count = await db.execute(select(func.count()).select_from(GitHubProject))
        gh_total = gh_count.scalar()

        gh_latest = await db.execute(
            select(GitHubProject.created_at)
            .order_by(GitHubProject.created_at.desc())
            .limit(1)
        )
        gh_last_update = gh_latest.scalar_one_or_none()

        categories_status["github"] = {
            "name": "GitHub í”„ë¡œì íŠ¸",
            "icon": "â­",
            "total": gh_total,
            "last_update": gh_last_update.isoformat() if gh_last_update else None,
            "status": "healthy" if gh_total > 0 else "no_data"
        }
    except Exception as e:
        categories_status["github"] = {
            "name": "GitHub í”„ë¡œì íŠ¸",
            "icon": "â­",
            "total": 0,
            "last_update": None,
            "status": "error",
            "error": str(e)
        }

    # YouTube Videos
    try:
        yt_count = await db.execute(select(func.count()).select_from(YouTubeVideo))
        yt_total = yt_count.scalar()

        yt_latest = await db.execute(
            select(YouTubeVideo.created_at)
            .order_by(YouTubeVideo.created_at.desc())
            .limit(1)
        )
        yt_last_update = yt_latest.scalar_one_or_none()

        categories_status["youtube"] = {
            "name": "YouTube ì˜ìƒ",
            "icon": "ğŸ“º",
            "total": yt_total,
            "last_update": yt_last_update.isoformat() if yt_last_update else None,
            "status": "healthy" if yt_total > 0 else "no_data"
        }
    except Exception as e:
        categories_status["youtube"] = {
            "name": "YouTube ì˜ìƒ",
            "icon": "ğŸ“º",
            "total": 0,
            "last_update": None,
            "status": "error",
            "error": str(e)
        }

    # AI Papers
    try:
        paper_count = await db.execute(select(func.count()).select_from(AIPaper))
        paper_total = paper_count.scalar()

        paper_latest = await db.execute(
            select(AIPaper.created_at)
            .order_by(AIPaper.created_at.desc())
            .limit(1)
        )
        paper_last_update = paper_latest.scalar_one_or_none()

        categories_status["papers"] = {
            "name": "AI ë…¼ë¬¸",
            "icon": "ğŸ“„",
            "total": paper_total,
            "last_update": paper_last_update.isoformat() if paper_last_update else None,
            "status": "healthy" if paper_total > 0 else "no_data"
        }
    except Exception as e:
        categories_status["papers"] = {
            "name": "AI ë…¼ë¬¸",
            "icon": "ğŸ“„",
            "total": 0,
            "last_update": None,
            "status": "error",
            "error": str(e)
        }

    # AI News
    try:
        news_count = await db.execute(select(func.count()).select_from(AINews))
        news_total = news_count.scalar()

        news_latest = await db.execute(
            select(AINews.created_at)
            .order_by(AINews.created_at.desc())
            .limit(1)
        )
        news_last_update = news_latest.scalar_one_or_none()

        categories_status["news"] = {
            "name": "AI ë‰´ìŠ¤",
            "icon": "ğŸ“°",
            "total": news_total,
            "last_update": news_last_update.isoformat() if news_last_update else None,
            "status": "healthy" if news_total > 0 else "no_data"
        }
    except Exception as e:
        categories_status["news"] = {
            "name": "AI ë‰´ìŠ¤",
            "icon": "ğŸ“°",
            "total": 0,
            "last_update": None,
            "status": "error",
            "error": str(e)
        }

    # AI Conferences
    try:
        conf_count = await db.execute(select(func.count()).select_from(AIConference))
        conf_total = conf_count.scalar()

        conf_latest = await db.execute(
            select(AIConference.created_at)
            .order_by(AIConference.created_at.desc())
            .limit(1)
        )
        conf_last_update = conf_latest.scalar_one_or_none()

        categories_status["conferences"] = {
            "name": "AI ì»¨í¼ëŸ°ìŠ¤",
            "icon": "ğŸ“…",
            "total": conf_total,
            "last_update": conf_last_update.isoformat() if conf_last_update else None,
            "status": "healthy" if conf_total > 0 else "no_data"
        }
    except Exception as e:
        categories_status["conferences"] = {
            "name": "AI ì»¨í¼ëŸ°ìŠ¤",
            "icon": "ğŸ“…",
            "total": 0,
            "last_update": None,
            "status": "error",
            "error": str(e)
        }

    # AI Tools
    try:
        tool_count = await db.execute(select(func.count()).select_from(AITool))
        tool_total = tool_count.scalar()

        tool_latest = await db.execute(
            select(AITool.created_at)
            .order_by(AITool.created_at.desc())
            .limit(1)
        )
        tool_last_update = tool_latest.scalar_one_or_none()

        categories_status["tools"] = {
            "name": "AI ë„êµ¬",
            "icon": "ğŸ› ï¸",
            "total": tool_total,
            "last_update": tool_last_update.isoformat() if tool_last_update else None,
            "status": "healthy" if tool_total > 0 else "no_data"
        }
    except Exception as e:
        categories_status["tools"] = {
            "name": "AI ë„êµ¬",
            "icon": "ğŸ› ï¸",
            "total": 0,
            "last_update": None,
            "status": "error",
            "error": str(e)
        }

    # AI Jobs
    try:
        job_count = await db.execute(select(func.count()).select_from(AIJobTrend))
        job_total = job_count.scalar()

        job_latest = await db.execute(
            select(AIJobTrend.created_at)
            .order_by(AIJobTrend.created_at.desc())
            .limit(1)
        )
        job_last_update = job_latest.scalar_one_or_none()

        categories_status["jobs"] = {
            "name": "AI ì±„ìš©",
            "icon": "ğŸ’¼",
            "total": job_total,
            "last_update": job_last_update.isoformat() if job_last_update else None,
            "status": "healthy" if job_total > 0 else "no_data"
        }
    except Exception as e:
        categories_status["jobs"] = {
            "name": "AI ì±„ìš©",
            "icon": "ğŸ’¼",
            "total": 0,
            "last_update": None,
            "status": "error",
            "error": str(e)
        }

    # AI Policies
    try:
        policy_count = await db.execute(select(func.count()).select_from(AIPolicy))
        policy_total = policy_count.scalar()

        policy_latest = await db.execute(
            select(AIPolicy.created_at)
            .order_by(AIPolicy.created_at.desc())
            .limit(1)
        )
        policy_last_update = policy_latest.scalar_one_or_none()

        categories_status["policies"] = {
            "name": "AI ì •ì±…",
            "icon": "ğŸ“œ",
            "total": policy_total,
            "last_update": policy_last_update.isoformat() if policy_last_update else None,
            "status": "healthy" if policy_total > 0 else "no_data"
        }
    except Exception as e:
        categories_status["policies"] = {
            "name": "AI ì •ì±…",
            "icon": "ğŸ“œ",
            "total": 0,
            "last_update": None,
            "status": "error",
            "error": str(e)
        }

    # Overall system status
    total_items = sum(cat.get("total", 0) for cat in categories_status.values())
    healthy_categories = sum(1 for cat in categories_status.values() if cat.get("status") == "healthy")
    now_iso = datetime.utcnow().isoformat()

    # DB size (ê°€ëŠ¥í•œ ê²½ìš°)
    db_size = "unknown"
    try:
        db_size_query = await db.execute(
            text("SELECT pg_size_pretty(pg_database_size(current_database()))")
        )
        db_size = db_size_query.scalar() or "unknown"
    except Exception:
        pass

    # Redis connectivity
    redis_running = False
    try:
        redis = await get_redis()
        redis_running = bool(await redis.ping())
    except Exception:
        redis_running = False

    # Scheduler runtime metrics
    runtime = get_scheduler_runtime_status()
    scheduler_jobs = []
    next_run_candidates = []
    completed_today = 0
    failed_today = 0
    today = datetime.utcnow().date()

    for job in scheduler.get_jobs():
        meta = runtime.get(job.id, {})
        next_run = job.next_run_time.isoformat() if job.next_run_time else None
        last_run = meta.get("last_run")
        if next_run:
            next_run_candidates.append(next_run)
        if last_run:
            try:
                parsed_last = datetime.fromisoformat(last_run).date()
                if parsed_last == today:
                    if meta.get("last_status") == "success":
                        completed_today += 1
                    elif meta.get("last_status") == "error":
                        failed_today += 1
            except Exception:
                pass

        scheduler_jobs.append(
            {
                "id": job.id,
                "name": job.name,
                "next_run": next_run,
                "last_run": last_run,
                "last_status": meta.get("last_status"),
                "last_error": meta.get("last_error"),
            }
        )

    last_crawl = None
    if runtime:
        last_runs = [v.get("last_run") for v in runtime.values() if v.get("last_run")]
        if last_runs:
            last_crawl = sorted(last_runs)[-1]

    next_crawl = sorted(next_run_candidates)[0] if next_run_candidates else None

    if db_connected and healthy_categories == len(categories_status):
        status = "healthy"
    elif db_connected:
        status = "degraded"
    else:
        status = "down"

    services = [
        {
            "name": "backend",
            "status": "running",
            "last_check": now_iso,
        },
        {
            "name": "database",
            "status": "running" if db_connected else "error",
            "last_check": now_iso,
        },
        {
            "name": "scheduler",
            "status": "running" if scheduler.running else "stopped",
            "last_check": now_iso,
        },
        {
            "name": "redis-cache",
            "status": "running" if redis_running else "error",
            "last_check": now_iso,
        },
    ]

    response = {
        "status": status,
        "uptime": "unknown",
        "version": settings.app_version,
        "services": services,
        "last_crawl": last_crawl,
        "next_crawl": next_crawl,
        "database": {
            "status": "connected" if db_connected else "disconnected",
            "size": db_size,
            "collections": len(categories_status),
        },
        "crawler": {
            "status": "running" if scheduler.running else "stopped",
            "active_jobs": len(scheduler.get_jobs()) if scheduler.running else 0,
            "completed_today": completed_today,
            "failed_today": failed_today,
        },
        "scheduler_jobs": scheduler_jobs,
        # backward-compat fields
        "backend_status": "online",
        "database_status": "connected" if db_connected else "disconnected",
        "timestamp": now_iso,
        "total_items": total_items,
        "healthy_categories": healthy_categories,
        "total_categories": len(categories_status),
        "categories": categories_status,
    }
    await cache_set(cache_key, response, ttl=TTL_SYSTEM_STATUS)
    return response


@router.get("/keywords")
async def get_keywords(
    db: AsyncSession = Depends(get_db),
    limit: int = 50
) -> Dict[str, Any]:
    """
    ì „ì²´ ì¹´í…Œê³ ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì§‘ê³„

    - ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ keywords í•„ë“œë¥¼ í•©ì‚°
    - ë¹ˆë„ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    - ì›Œë“œ í´ë¼ìš°ë“œ ë° í‚¤ì›Œë“œ ìˆœìœ„ìš© ë°ì´í„° ì œê³µ
    """

    cache_key = f"system:keywords:{limit}"
    cached = await cache_get(cache_key)
    if cached is not None:
        return cached

    all_keywords = []

    # Hugging Face keywords
    try:
        hf_result = await db.execute(
            select(HuggingFaceModel.key_features).where(
                HuggingFaceModel.key_features.isnot(None)
            ).limit(100)
        )
        for row in hf_result.scalars():
            if row and isinstance(row, list):
                all_keywords.extend(row)
    except Exception:
        pass

    # GitHub keywords
    try:
        gh_result = await db.execute(
            select(GitHubProject.keywords).where(
                GitHubProject.keywords.isnot(None)
            ).limit(100)
        )
        for row in gh_result.scalars():
            if row and isinstance(row, list):
                all_keywords.extend(row)
    except Exception:
        pass

    # YouTube keywords
    try:
        yt_result = await db.execute(
            select(YouTubeVideo.keywords).where(
                YouTubeVideo.keywords.isnot(None)
            ).limit(100)
        )
        for row in yt_result.scalars():
            if row and isinstance(row, list):
                all_keywords.extend(row)
    except Exception:
        pass

    # AI Papers keywords
    try:
        paper_result = await db.execute(
            select(AIPaper.keywords).where(
                AIPaper.keywords.isnot(None)
            ).limit(100)
        )
        for row in paper_result.scalars():
            if row and isinstance(row, list):
                all_keywords.extend(row)
    except Exception:
        pass

    # AI News keywords
    try:
        news_result = await db.execute(
            select(AINews.keywords).where(
                AINews.keywords.isnot(None)
            ).limit(100)
        )
        for row in news_result.scalars():
            if row and isinstance(row, list):
                all_keywords.extend(row)
    except Exception:
        pass

    # AI Conference keywords
    try:
        conf_result = await db.execute(
            select(AIConference.topics).where(
                AIConference.topics.isnot(None)
            ).limit(100)
        )
        for row in conf_result.scalars():
            if row and isinstance(row, list):
                all_keywords.extend(row)
    except Exception:
        pass

    # AI Tool keywords
    try:
        tool_result = await db.execute(
            select(AITool.key_features).where(
                AITool.key_features.isnot(None)
            ).limit(100)
        )
        for row in tool_result.scalars():
            if row and isinstance(row, list):
                all_keywords.extend(row)
    except Exception:
        pass

    # AI Job keywords
    try:
        job_result = await db.execute(
            select(AIJobTrend.required_skills).where(
                AIJobTrend.required_skills.isnot(None)
            ).limit(100)
        )
        for row in job_result.scalars():
            if row and isinstance(row, list):
                all_keywords.extend(row)
    except Exception:
        pass

    # AI Policy keywords
    try:
        policy_result = await db.execute(
            select(AIPolicy.impact_areas).where(
                AIPolicy.impact_areas.isnot(None)
            ).limit(100)
        )
        for row in policy_result.scalars():
            if row and isinstance(row, list):
                all_keywords.extend(row)
    except Exception:
        pass

    # Count keywords
    if not all_keywords:
        payload = {
            "total_keywords": 0,
            "unique_keywords": 0,
            "top_keywords": [],
            "all_keywords": []
        }
        await cache_set(cache_key, payload, ttl=TTL_KEYWORDS)
        return payload

    keyword_counts = Counter(all_keywords)

    # Top keywords with counts
    top_keywords = [
        {"keyword": keyword, "count": count}
        for keyword, count in keyword_counts.most_common(limit)
    ]

    # All keywords for word cloud (with normalized counts)
    max_count = max(keyword_counts.values()) if keyword_counts else 1
    all_keywords_normalized = [
        {
            "keyword": keyword,
            "count": count,
            "weight": count / max_count
        }
        for keyword, count in keyword_counts.items()
    ]

    payload = {
        "total_keywords": len(all_keywords),
        "unique_keywords": len(keyword_counts),
        "top_keywords": top_keywords,
        "all_keywords": all_keywords_normalized[:limit]
    }
    await cache_set(cache_key, payload, ttl=TTL_KEYWORDS)
    return payload


@router.get("/collection-logs")
async def get_collection_logs() -> Dict[str, Any]:
    """ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ ë¡œê·¸ ìš”ì•½ (ìŠ¤ì¼€ì¤„ëŸ¬ ëŸ°íƒ€ì„ ìƒíƒœ ê¸°ë°˜)."""
    runtime = get_scheduler_runtime_status()
    jobs = []
    success_count = 0
    error_count = 0

    for job in scheduler.get_jobs():
        meta = runtime.get(job.id, {})
        last_status = meta.get("last_status")
        if last_status == "success":
            success_count += 1
        elif last_status == "error":
            error_count += 1

        jobs.append(
            {
                "id": job.id,
                "name": job.name,
                "next_run": job.next_run_time.isoformat() if job.next_run_time else None,
                "last_run": meta.get("last_run"),
                "last_status": last_status,
                "last_error": meta.get("last_error"),
            }
        )

    return {
        "timestamp": datetime.utcnow().isoformat(),
        "total_jobs": len(jobs),
        "successful_jobs": success_count,
        "failed_jobs": error_count,
        "jobs": jobs,
    }


@router.get("/logs")
async def get_logs(
    log_type: str = Query("app", description="ë¡œê·¸ íƒ€ì…: app, error, collection"),
    lines: int = Query(100, ge=1, le=1000, description="ì½ì„ ë¼ì¸ ìˆ˜"),
) -> Dict[str, Any]:
    """
    ë¡œê·¸ íŒŒì¼ ë‚´ìš© ì¡°íšŒ

    - **log_type**: ë¡œê·¸ íƒ€ì… (app, error, collection)
    - **lines**: ì½ì„ ë¼ì¸ ìˆ˜ (ê¸°ë³¸ê°’: 100, ìµœëŒ€: 1000)
    """

    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ë§¤í•‘
    log_files = {
        "app": Path("logs/app.log"),
        "error": Path("logs/error.log"),
        "collection": Path("logs/collection.log")
    }

    if log_type not in log_files:
        raise HTTPException(status_code=400, detail=f"Invalid log type. Must be one of: {', '.join(log_files.keys())}")

    log_file = log_files[log_type]

    if not log_file.exists():
        return {
            "log_type": log_type,
            "file_path": str(log_file),
            "exists": False,
            "lines": [],
            "total_lines": 0,
            "message": "ë¡œê·¸ íŒŒì¼ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        }

    try:
        # íŒŒì¼ì˜ ë§ˆì§€ë§‰ Nì¤„ ì½ê¸°
        with open(log_file, 'r', encoding='utf-8') as f:
            all_lines = f.readlines()

        # ë§ˆì§€ë§‰ Nì¤„ë§Œ ê°€ì ¸ì˜¤ê¸°
        recent_lines = all_lines[-lines:] if len(all_lines) > lines else all_lines

        return {
            "log_type": log_type,
            "file_path": str(log_file),
            "exists": True,
            "lines": [line.rstrip() for line in recent_lines],
            "total_lines": len(all_lines),
            "returned_lines": len(recent_lines),
            "file_size_kb": round(log_file.stat().st_size / 1024, 2)
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¡œê·¸ ì½ê¸° ì‹¤íŒ¨: {str(e)}")


@router.get("/logs/list")
async def list_log_files() -> Dict[str, Any]:
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ë¡œê·¸ íŒŒì¼ ëª©ë¡ ì¡°íšŒ
    """

    logs_dir = Path("logs")

    if not logs_dir.exists():
        return {
            "logs_directory": str(logs_dir),
            "exists": False,
            "log_files": []
        }

    log_files = []
    for log_file in logs_dir.glob("*.log*"):
        try:
            stat = log_file.stat()
            log_files.append({
                "filename": log_file.name,
                "path": str(log_file),
                "size_kb": round(stat.st_size / 1024, 2),
                "modified": datetime.fromtimestamp(stat.st_mtime).isoformat()
            })
        except Exception:
            continue

    # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
    log_files.sort(key=lambda x: x["modified"], reverse=True)

    return {
        "logs_directory": str(logs_dir.absolute()),
        "exists": True,
        "total_files": len(log_files),
        "log_files": log_files
    }


@router.post("/collect")
async def trigger_data_collection(background_tasks: BackgroundTasks) -> Dict[str, Any]:
    """
    ìˆ˜ë™ìœ¼ë¡œ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ íŠ¸ë¦¬ê±° (ë°±ê·¸ë¼ìš´ë“œ)

    - ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ë°ì´í„°ë¥¼ ì¦‰ì‹œ ìˆ˜ì§‘í•©ë‹ˆë‹¤
    - ì´ ì‘ì—…ì€ 1-2ë¶„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤
    - ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ì¦‰ì‹œ ì‘ë‹µì´ ë°˜í™˜ë©ë‹ˆë‹¤
    """

    # FastAPI BackgroundTasksë¡œ ì‹¤í–‰
    background_tasks.add_task(collect_all_data)

    return {
        "status": "started",
        "message": "ë°ì´í„° ìˆ˜ì§‘ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. 1-2ë¶„ í›„ ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.",
        "timestamp": datetime.utcnow().isoformat()
    }


@router.post("/collect/sync")
async def trigger_data_collection_sync() -> Dict[str, Any]:
    """
    ìˆ˜ë™ìœ¼ë¡œ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ íŠ¸ë¦¬ê±° (ë™ê¸° - ì™„ë£Œê¹Œì§€ ëŒ€ê¸°)

    - ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ë°ì´í„°ë¥¼ ì¦‰ì‹œ ìˆ˜ì§‘í•©ë‹ˆë‹¤
    - ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°í•˜ë¯€ë¡œ 1-2ë¶„ ì†Œìš”ë©ë‹ˆë‹¤
    - ì—ëŸ¬ ë°œìƒ ì‹œ ì¦‰ì‹œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤
    """

    try:
        # ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ (ì™„ë£Œê¹Œì§€ ëŒ€ê¸°)
        await collect_all_data()

        return {
            "status": "completed",
            "message": "ë°ì´í„° ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            "timestamp": datetime.utcnow().isoformat(),
            "error": str(e)
        }
