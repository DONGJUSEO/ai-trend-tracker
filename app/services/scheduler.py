"""ìŠ¤ì¼€ì¤„ëŸ¬ ì„œë¹„ìŠ¤ - ì •ê¸°ì  ë°ì´í„° ìˆ˜ì§‘"""
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.events import EVENT_JOB_EXECUTED, EVENT_JOB_ERROR
from datetime import datetime, timedelta, timezone
import asyncio
import logging

from app.config import get_settings
from app.database import AsyncSessionLocal
from app.services.huggingface_service import HuggingFaceService
from app.services.youtube_service import YouTubeService
from app.services.arxiv_service import ArxivService
from app.services.news_service import NewsService
from app.services.github_service import GitHubService
from app.services.conference_service import ConferenceService
from app.services.ai_tool_service import AIToolService
from app.services.job_trend_service import JobTrendService
from app.services.policy_service import PolicyService
from app.services.ai_summary_service import AISummaryService
from app.services.trending_keyword_service import ExternalTrendingKeywordService
from app.models.huggingface import HuggingFaceModel
from app.models.youtube import YouTubeVideo
from app.models.paper import AIPaper
from app.models.news import AINews
from app.models.github import GitHubProject
from app.models.conference import AIConference
from app.models.ai_tool import AITool
from app.models.job_trend import AIJobTrend
from app.models.policy import AIPolicy
from sqlalchemy import select, desc, update
from app.cache import cache_delete_pattern
from app.services.notification_service import send_error_webhook
from app.db_compat import has_columns

settings = get_settings()

# ìŠ¤ì¼€ì¤„ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
scheduler = AsyncIOScheduler()

# ì‘ì—…ë³„ ìµœê·¼ ì‹¤í–‰ ìƒíƒœ (System APIì—ì„œ ì‚¬ìš©)
JOB_RUNTIME_STATUS = {}


async def _invalidate_cache_after_collection(job_id: str):
    """ìˆ˜ì§‘ ì™„ë£Œ í›„ ëŒ€ì‹œë³´ë“œ/ê²€ìƒ‰ ìºì‹œ ë¬´íš¨í™”."""
    await cache_delete_pattern("dashboard:*")
    await cache_delete_pattern("search:*")
    await cache_delete_pattern("system:*")
    await cache_delete_pattern("list:*")
    print(f"ğŸ§¹ ìºì‹œ ë¬´íš¨í™” ì™„ë£Œ ({job_id})")


def _scheduler_event_listener(event):
    """APScheduler ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ: ë§ˆì§€ë§‰ ì‹¤í–‰/ì„±ê³µ/ì‹¤íŒ¨ ìƒíƒœ ê¸°ë¡."""
    job_id = getattr(event, "job_id", "unknown")
    now = datetime.now(timezone.utc).isoformat()
    if getattr(event, "exception", None):
        error_message = str(event.exception)
        JOB_RUNTIME_STATUS[job_id] = {
            "last_run": now,
            "last_status": "error",
            "last_error": error_message,
        }
        try:
            loop = asyncio.get_running_loop()
            loop.create_task(
                send_error_webhook(
                    title="Scheduler Job Failed",
                    job_id=job_id,
                    message=error_message,
                )
            )
        except RuntimeError:
            pass
    else:
        JOB_RUNTIME_STATUS[job_id] = {
            "last_run": now,
            "last_status": "success",
            "last_error": None,
        }
        try:
            loop = asyncio.get_running_loop()
            loop.create_task(_invalidate_cache_after_collection(job_id))
        except RuntimeError:
            pass


def get_scheduler_runtime_status():
    """ì‘ì—…ë³„ ëŸ°íƒ€ì„ ìƒíƒœ ë°˜í™˜."""
    return JOB_RUNTIME_STATUS


async def archive_old_data(days: int = 30):
    """30ì¼ ì´ˆê³¼ ë°ì´í„° soft archive."""
    cutoff = datetime.now(timezone.utc) - timedelta(days=days)
    archived_at = datetime.now(timezone.utc)
    print(
        f"\nğŸ—„ï¸  ì•„ì¹´ì´ë¸Œ ì‘ì—… ì‹œì‘ (cutoff={cutoff.strftime('%Y-%m-%d %H:%M:%S')} UTC)"
    )

    archived_summary = {}
    model_specs = [
        ("huggingface", "huggingface_models", HuggingFaceModel, HuggingFaceModel.collected_at),
        ("youtube", "youtube_videos", YouTubeVideo, YouTubeVideo.published_at),
        ("papers", "ai_papers", AIPaper, AIPaper.published_date),
        ("news", "ai_news", AINews, AINews.published_date),
        ("github", "github_projects", GitHubProject, GitHubProject.pushed_at),
        ("conferences", "ai_conferences", AIConference, AIConference.end_date),
        ("tools", "ai_tools", AITool, AITool.created_at),
        ("jobs", "ai_job_trends", AIJobTrend, AIJobTrend.posted_date),
        ("policies", "ai_policies", AIPolicy, AIPolicy.effective_date),
    ]

    async with AsyncSessionLocal() as db:
        try:
            for name, table_name, model, date_col in model_specs:
                column_flags = await has_columns(
                    db,
                    table_name,
                    ["is_archived", "archived_at"],
                )
                has_archive_columns = (
                    column_flags["is_archived"] and column_flags["archived_at"]
                )
                if not has_archive_columns:
                    archived_summary[name] = 0
                    continue

                value_payload = {
                    "is_archived": True,
                    "archived_at": archived_at,
                }
                if hasattr(model, "is_trending"):
                    value_payload["is_trending"] = False

                stmt = (
                    update(model)
                    .where(
                        date_col.isnot(None),
                        date_col < cutoff,
                        model.is_archived.is_(False),
                    )
                    .values(**value_payload)
                )
                result = await db.execute(stmt)
                archived_summary[name] = result.rowcount or 0

            await db.commit()
        except Exception as e:
            await db.rollback()
            print(f"âŒ ì•„ì¹´ì´ë¸Œ ì‘ì—… ì‹¤íŒ¨: {e}")
            raise

    total_archived = sum(archived_summary.values())
    details = ", ".join(f"{k}={v}" for k, v in archived_summary.items())
    print(f"âœ… ì•„ì¹´ì´ë¸Œ ì™„ë£Œ: total={total_archived} ({details})")
    await _invalidate_cache_after_collection("archive_old_data")


async def collect_huggingface_data():
    """Hugging Face ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…"""
    print(f"\n{'='*60}")
    print(f"ğŸ¤– ìë™ ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    async with AsyncSessionLocal() as db:
        try:
            # 1. íŠ¸ë Œë”© ëª¨ë¸ ìˆ˜ì§‘
            hf_service = HuggingFaceService()
            result = await hf_service.collect_trending_models(db, limit=50)

            if result["success"]:
                print(f"âœ… Hugging Face: {result['count']}ê°œ ì‹ ê·œ ëª¨ë¸ ì €ì¥")
            else:
                print("âš ï¸  Hugging Face ìˆ˜ì§‘ ì‹¤íŒ¨")

            # 2. AI ìš”ì•½ ìƒì„± (ìš”ì•½ì´ ì—†ëŠ” ëª¨ë¸ë“¤ì— ëŒ€í•´)
            ai_service = AISummaryService()
            if ai_service.model:  # API í‚¤ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                query = select(HuggingFaceModel).where(
                    HuggingFaceModel.summary == None
                ).limit(10)  # í•œë²ˆì— 10ê°œì”©
                result = await db.execute(query)
                models_without_summary = result.scalars().all()

                if models_without_summary:
                    print(f"\nğŸ§  AI ìš”ì•½ ìƒì„± ì‹œì‘ ({len(models_without_summary)}ê°œ ëª¨ë¸)...")

                    for model in models_without_summary:
                        try:
                            summary_data = await ai_service.summarize_huggingface_model(
                                model_name=model.model_name,
                                description=model.description,
                                task=model.task,
                                tags=model.tags or [],
                            )

                            if summary_data["summary"]:
                                model.summary = summary_data["summary"]
                                model.key_features = summary_data["key_features"]
                                model.use_cases = summary_data["use_cases"]
                                print(f"  âœ… {model.model_name[:40]} - ìš”ì•½ ì™„ë£Œ")
                            else:
                                print(f"  âš ï¸  {model.model_name[:40]} - ìš”ì•½ ì‹¤íŒ¨")

                            # API í˜¸ì¶œ ì œí•œ íšŒí”¼ (ë¬´ë£Œ í‹°ì–´)
                            await asyncio.sleep(2)

                        except Exception as e:
                            print(f"  âŒ {model.model_name[:40]} - ì—ëŸ¬: {e}")
                            continue

                    await db.commit()
                    print(f"âœ… AI ìš”ì•½ ì™„ë£Œ")
            else:
                print("âš ï¸  Gemini API í‚¤ê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤")

        except Exception as e:
            print(f"âŒ ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            await db.close()

    await _invalidate_cache_after_collection("collect_huggingface")

    print(f"\n{'='*60}")
    print(f"âœ¨ ìë™ ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")


async def collect_youtube_data():
    """YouTube ë°ì´í„° ìˆ˜ì§‘ ì‘ì—… (íë ˆì´ì…˜ ì±„ë„ + í‚¤ì›Œë“œ ê²€ìƒ‰)"""
    print(f"\n{'='*60}")
    print(f"ğŸ“º YouTube ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    async with AsyncSessionLocal() as db:
        try:
            from app.models.youtube_channel import YouTubeChannel

            yt_service = YouTubeService()
            total_saved = 0

            # 1. íë ˆì´ì…˜ëœ ì±„ë„ì˜ ìµœì‹  ì˜ìƒ ìˆ˜ì§‘ (ìš°ì„ ìˆœìœ„ ë†’ì€ ìˆœ)
            print("ğŸ“Œ íë ˆì´ì…˜ëœ AI ìœ íŠœë²„ ì±„ë„ì—ì„œ ìµœì‹  ì˜ìƒ ìˆ˜ì§‘ ì¤‘...")
            result = await db.execute(
                select(YouTubeChannel)
                .where(YouTubeChannel.is_active == True)
                .order_by(desc(YouTubeChannel.priority))
                .limit(30)  # ìµœëŒ€ 30ê°œ ì±„ë„
            )
            channels = result.scalars().all()

            channel_videos_count = 0
            for channel in channels:
                try:
                    category_text = (channel.category or "").lower()
                    is_korean_channel = any(
                        token in category_text
                        for token in ["êµ­ë‚´", "korean", "ko"]
                    )
                    channel_lang = "ko" if is_korean_channel else "en"
                    videos = await yt_service.get_channel_videos(
                        channel_id=channel.channel_id,
                        max_results=15,  # ì±„ë„ë‹¹ ìµœì‹  15ê°œ
                        order="date",
                        relevance_language=channel_lang,
                        default_language=channel_lang,
                    )

                    if videos:
                        saved = await yt_service.save_videos_to_db(videos, db)
                        channel_videos_count += saved
                        if saved > 0:
                            print(
                                f"  âœ… {channel.channel_name}: {saved}ê°œ ì‹ ê·œ ì˜ìƒ"
                            )

                        # ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì‹œê°„ ì—…ë°ì´íŠ¸
                        channel.last_collected_at = datetime.now()
                        await db.commit()

                    await asyncio.sleep(0.5)  # API í˜¸ì¶œ ì œí•œ íšŒí”¼

                except Exception as e:
                    print(f"  âŒ {channel.channel_name}: {e}")
                    continue

            print(
                f"âœ… íë ˆì´ì…˜ ì±„ë„: {channel_videos_count}ê°œ ì‹ ê·œ ì˜ìƒ ì €ì¥\n"
            )

            # 2. í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ì¶”ê°€ AI íŠ¸ë Œë“œ ì˜ìƒ ìˆ˜ì§‘
            print("ğŸ“Œ í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ì¶”ê°€ AI íŠ¸ë Œë“œ ì˜ìƒ ìˆ˜ì§‘ ì¤‘...")
            queries = [
                ("AI artificial intelligence tutorial 2026", "en"),
                ("machine learning explained", "en"),
                ("deep learning tutorial", "en"),
                ("ChatGPT GPT-4", "en"),
                ("stable diffusion AI art", "en"),
                ("ì¸ê³µì§€ëŠ¥ ê°œë°œ íŠœí† ë¦¬ì–¼", "ko"),
                ("LLM ì‹¤ë¬´ í™œìš©", "ko"),
            ]

            keyword_videos_count = 0
            for query, lang in queries:
                videos = await yt_service.search_ai_videos(
                    query=query,
                    max_results=15,
                    order="viewCount",
                    relevance_language=lang,
                )

                if videos:
                    saved = await yt_service.save_videos_to_db(videos, db)
                    keyword_videos_count += saved
                    if saved > 0:
                        print(f"  âœ… '{query}': {saved}ê°œ ì‹ ê·œ ë¹„ë””ì˜¤")

                await asyncio.sleep(1)  # API í˜¸ì¶œ ì œí•œ íšŒí”¼

            print(f"âœ… í‚¤ì›Œë“œ ê²€ìƒ‰: {keyword_videos_count}ê°œ ì‹ ê·œ ì˜ìƒ ì €ì¥")

            total_saved = channel_videos_count + keyword_videos_count
            print(f"\nâœ… YouTube ì „ì²´: ì´ {total_saved}ê°œ ì‹ ê·œ ë¹„ë””ì˜¤ ì €ì¥")

            # 2. AI ìš”ì•½ ìƒì„± (ìš”ì•½ì´ ì—†ëŠ” ë¹„ë””ì˜¤ë“¤ì— ëŒ€í•´)
            ai_service = AISummaryService()
            if ai_service.model:  # API í‚¤ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                query = select(YouTubeVideo).where(
                    YouTubeVideo.summary == None
                ).limit(10)  # í•œë²ˆì— 10ê°œì”©
                result = await db.execute(query)
                videos_without_summary = result.scalars().all()

                if videos_without_summary:
                    print(f"\nğŸ§  AI ìš”ì•½ ìƒì„± ì‹œì‘ ({len(videos_without_summary)}ê°œ ë¹„ë””ì˜¤)...")

                    for video in videos_without_summary:
                        try:
                            summary_data = await ai_service.summarize_youtube_video(
                                title=video.title,
                                description=video.description,
                                tags=video.tags or [],
                            )

                            if summary_data["summary"]:
                                video.summary = summary_data["summary"]
                                video.keywords = summary_data["keywords"]
                                video.key_points = summary_data["key_points"]
                                print(f"  âœ… {video.title[:40]} - ìš”ì•½ ì™„ë£Œ")
                            else:
                                print(f"  âš ï¸  {video.title[:40]} - ìš”ì•½ ì‹¤íŒ¨")

                            # API í˜¸ì¶œ ì œí•œ íšŒí”¼
                            await asyncio.sleep(2)

                        except Exception as e:
                            print(f"  âŒ {video.title[:40]} - ì—ëŸ¬: {e}")
                            continue

                    await db.commit()
                    print(f"âœ… AI ìš”ì•½ ì™„ë£Œ")
            else:
                print("âš ï¸  Gemini API í‚¤ê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤")

        except Exception as e:
            print(f"âŒ YouTube ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            await db.close()

    await _invalidate_cache_after_collection("collect_youtube")

    print(f"\n{'='*60}")
    print(f"âœ¨ YouTube ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")


async def collect_papers_data():
    """AI Papers ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…"""
    print(f"\n{'='*60}")
    print(f"ğŸ“„ AI Papers ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    async with AsyncSessionLocal() as db:
        try:
            # 1. arXivì—ì„œ ìµœê·¼ ë…¼ë¬¸ ê²€ìƒ‰
            arxiv_service = ArxivService()

            # ìµœê·¼ 7ì¼ê°„ì˜ AI ë…¼ë¬¸ ìˆ˜ì§‘
            papers = await arxiv_service.search_recent_papers(
                days=7, max_results=100
            )

            if papers:
                saved = await arxiv_service.save_papers_to_db(papers, db)
                print(f"âœ… arXiv: {saved}ê°œ ì‹ ê·œ ë…¼ë¬¸ ì €ì¥")
            else:
                print("âš ï¸  arXivì—ì„œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 2. AI ìš”ì•½ ìƒì„± (ìš”ì•½ì´ ì—†ëŠ” ë…¼ë¬¸ë“¤ì— ëŒ€í•´)
            ai_service = AISummaryService()
            if ai_service.model:  # API í‚¤ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                query = select(AIPaper).where(
                    AIPaper.summary == None
                ).limit(10)  # í•œë²ˆì— 10ê°œì”©
                result = await db.execute(query)
                papers_without_summary = result.scalars().all()

                if papers_without_summary:
                    print(f"\nğŸ§  AI ìš”ì•½ ìƒì„± ì‹œì‘ ({len(papers_without_summary)}ê°œ ë…¼ë¬¸)...")

                    for paper in papers_without_summary:
                        try:
                            summary_data = await ai_service.summarize_paper(
                                title=paper.title,
                                abstract=paper.abstract,
                                authors=paper.authors or [],
                                categories=paper.categories or [],
                            )

                            if summary_data["summary"]:
                                paper.summary = summary_data["summary"]
                                paper.keywords = summary_data["keywords"]
                                paper.key_contributions = summary_data["key_contributions"]
                                print(f"  âœ… {paper.title[:40]} - ìš”ì•½ ì™„ë£Œ")
                            else:
                                print(f"  âš ï¸  {paper.title[:40]} - ìš”ì•½ ì‹¤íŒ¨")

                            # API í˜¸ì¶œ ì œí•œ íšŒí”¼
                            await asyncio.sleep(2)

                        except Exception as e:
                            print(f"  âŒ {paper.title[:40]} - ì—ëŸ¬: {e}")
                            continue

                    await db.commit()
                    print(f"âœ… AI ìš”ì•½ ì™„ë£Œ")
            else:
                print("âš ï¸  Gemini API í‚¤ê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤")

        except Exception as e:
            print(f"âŒ Papers ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            await db.close()

    await _invalidate_cache_after_collection("collect_papers")

    print(f"\n{'='*60}")
    print(f"âœ¨ Papers ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")


async def collect_news_data():
    """AI News ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…"""
    print(f"\n{'='*60}")
    print(f"ğŸ“° AI News ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    async with AsyncSessionLocal() as db:
        try:
            # 1. RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
            news_service = NewsService()

            articles = await news_service.fetch_all_feeds()

            if articles:
                saved = await news_service.save_news_to_db(articles, db)
                print(f"\nâœ… AI News: ì´ {saved}ê°œ ì‹ ê·œ ë‰´ìŠ¤ ì €ì¥")
            else:
                print("âš ï¸  RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 2. AI ìš”ì•½ ìƒì„± (ìš”ì•½ì´ ì—†ëŠ” ë‰´ìŠ¤ë“¤ì— ëŒ€í•´)
            ai_service = AISummaryService()
            if ai_service.model:  # API í‚¤ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                query = select(AINews).where(
                    AINews.summary == None
                ).limit(10)  # í•œë²ˆì— 10ê°œì”©
                result = await db.execute(query)
                news_without_summary = result.scalars().all()

                if news_without_summary:
                    print(f"\nğŸ§  AI ìš”ì•½ ìƒì„± ì‹œì‘ ({len(news_without_summary)}ê°œ ë‰´ìŠ¤)...")

                    for news_item in news_without_summary:
                        try:
                            summary_data = await ai_service.summarize_news(
                                title=news_item.title,
                                content=news_item.content or news_item.excerpt,
                                source=news_item.source,
                            )

                            if summary_data["summary"]:
                                news_item.summary = summary_data["summary"]
                                news_item.keywords = summary_data["keywords"]
                                news_item.key_points = summary_data["key_points"]
                                print(f"  âœ… {news_item.title[:40]} - ìš”ì•½ ì™„ë£Œ")
                            else:
                                print(f"  âš ï¸  {news_item.title[:40]} - ìš”ì•½ ì‹¤íŒ¨")

                            # API í˜¸ì¶œ ì œí•œ íšŒí”¼
                            await asyncio.sleep(2)

                        except Exception as e:
                            print(f"  âŒ {news_item.title[:40]} - ì—ëŸ¬: {e}")
                            continue

                    await db.commit()
                    print(f"âœ… AI ìš”ì•½ ì™„ë£Œ")
            else:
                print("âš ï¸  Gemini API í‚¤ê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤")

        except Exception as e:
            print(f"âŒ News ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            await db.close()

    await _invalidate_cache_after_collection("collect_news")

    print(f"\n{'='*60}")
    print(f"âœ¨ News ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")


async def collect_github_data():
    """GitHub íŠ¸ë Œë”© í”„ë¡œì íŠ¸ ìˆ˜ì§‘ ì‘ì—…"""
    print(f"\n{'='*60}")
    print(f"â­ GitHub íŠ¸ë Œë”© ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    async with AsyncSessionLocal() as db:
        try:
            # 1. GitHubì—ì„œ íŠ¸ë Œë”© AI/ML í”„ë¡œì íŠ¸ ê²€ìƒ‰
            github_service = GitHubService()

            projects = await github_service.fetch_trending_repos(
                language="", max_results=50
            )

            if projects:
                saved = await github_service.save_projects_to_db(projects, db)
                print(f"âœ… GitHub: {saved}ê°œ ì‹ ê·œ í”„ë¡œì íŠ¸ ì €ì¥")
            else:
                print("âš ï¸  GitHubì—ì„œ í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 2. AI ìš”ì•½ ìƒì„± (ìš”ì•½ì´ ì—†ëŠ” í”„ë¡œì íŠ¸ë“¤ì— ëŒ€í•´)
            ai_service = AISummaryService()
            if ai_service.model:  # API í‚¤ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                query = select(GitHubProject).where(
                    GitHubProject.summary == None
                ).limit(10)  # í•œë²ˆì— 10ê°œì”©
                result = await db.execute(query)
                projects_without_summary = result.scalars().all()

                if projects_without_summary:
                    print(f"\nğŸ§  AI ìš”ì•½ ìƒì„± ì‹œì‘ ({len(projects_without_summary)}ê°œ í”„ë¡œì íŠ¸)...")

                    for project in projects_without_summary:
                        try:
                            summary_data = await ai_service.summarize_github_project(
                                repo_name=project.repo_name,
                                description=project.description,
                                language=project.language,
                                topics=project.topics or [],
                            )

                            if summary_data["summary"]:
                                project.summary = summary_data["summary"]
                                project.keywords = summary_data["keywords"]
                                project.use_cases = summary_data["use_cases"]
                                print(f"  âœ… {project.repo_name[:40]} - ìš”ì•½ ì™„ë£Œ")
                            else:
                                print(f"  âš ï¸  {project.repo_name[:40]} - ìš”ì•½ ì‹¤íŒ¨")

                            # API í˜¸ì¶œ ì œí•œ íšŒí”¼
                            await asyncio.sleep(2)

                        except Exception as e:
                            print(f"  âŒ {project.repo_name[:40]} - ì—ëŸ¬: {e}")
                            continue

                    await db.commit()
                    print(f"âœ… AI ìš”ì•½ ì™„ë£Œ")
            else:
                print("âš ï¸  Gemini API í‚¤ê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤")

        except Exception as e:
            print(f"âŒ GitHub ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            await db.close()

    await _invalidate_cache_after_collection("collect_github")

    print(f"\n{'='*60}")
    print(f"âœ¨ GitHub ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")


async def collect_conference_data():
    """AI Conference ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…"""
    print(f"\n{'='*60}")
    print(f"ğŸ“… AI Conference ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    async with AsyncSessionLocal() as db:
        try:
            # 1. WikiCFPì—ì„œ AI ì»¨í¼ëŸ°ìŠ¤ ìˆ˜ì§‘
            conference_service = ConferenceService()

            conferences = await conference_service.fetch_wikicfp_conferences(max_results=50)

            if conferences:
                saved = await conference_service.save_to_db(conferences, db)
                print(f"âœ… AI Conference: {saved}ê°œ ì‹ ê·œ ì»¨í¼ëŸ°ìŠ¤ ì €ì¥")
            else:
                print("âš ï¸  WikiCFPì—ì„œ ì»¨í¼ëŸ°ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 2. AI ìš”ì•½ ìƒì„± (ìš”ì•½ì´ ì—†ëŠ” ì»¨í¼ëŸ°ìŠ¤ë“¤ì— ëŒ€í•´)
            ai_service = AISummaryService()
            if ai_service.model:  # API í‚¤ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                query = select(AIConference).where(
                    AIConference.summary == None
                ).limit(10)  # í•œë²ˆì— 10ê°œì”©
                result = await db.execute(query)
                conferences_without_summary = result.scalars().all()

                if conferences_without_summary:
                    print(f"\nğŸ§  AI ìš”ì•½ ìƒì„± ì‹œì‘ ({len(conferences_without_summary)}ê°œ ì»¨í¼ëŸ°ìŠ¤)...")

                    for conference in conferences_without_summary:
                        try:
                            summary_data = await ai_service.summarize_conference(
                                name=conference.conference_name,
                                description=conference.summary or "",
                                topics=conference.topics or [],
                            )

                            if summary_data.get("summary"):
                                conference.summary = summary_data["summary"]
                                conference.keywords = summary_data.get("keywords", [])
                                print(f"  âœ… {conference.conference_name[:40]} - ìš”ì•½ ì™„ë£Œ")
                            else:
                                print(f"  âš ï¸  {conference.conference_name[:40]} - ìš”ì•½ ì‹¤íŒ¨")

                            # API í˜¸ì¶œ ì œí•œ íšŒí”¼
                            await asyncio.sleep(2)

                        except Exception as e:
                            print(f"  âŒ {conference.conference_name[:40]} - ì—ëŸ¬: {e}")
                            continue

                    await db.commit()
                    print(f"âœ… AI ìš”ì•½ ì™„ë£Œ")
            else:
                print("âš ï¸  Gemini API í‚¤ê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤")

        except Exception as e:
            print(f"âŒ Conference ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            await db.close()

    await _invalidate_cache_after_collection("collect_conferences")

    print(f"\n{'='*60}")
    print(f"âœ¨ Conference ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")


async def collect_tool_data():
    """AI Tool ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…"""
    print(f"\n{'='*60}")
    print(f"ğŸ› ï¸ AI Tool ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    async with AsyncSessionLocal() as db:
        try:
            # 1. íŠ¸ë Œë”© AI ë„êµ¬ ìˆ˜ì§‘
            tool_service = AIToolService()

            tools = await tool_service.fetch_trending_tools(max_results=30)

            if tools:
                saved = await tool_service.save_to_db(tools, db)
                print(f"âœ… AI Tool: {saved}ê°œ ì‹ ê·œ ë„êµ¬ ì €ì¥")
            else:
                print("âš ï¸  AI ë„êµ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 2. AI ìš”ì•½ ìƒì„± (ìš”ì•½ì´ ì—†ëŠ” ë„êµ¬ë“¤ì— ëŒ€í•´)
            ai_service = AISummaryService()
            if ai_service.model:  # API í‚¤ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                query = select(AITool).where(
                    AITool.summary == None
                ).limit(10)  # í•œë²ˆì— 10ê°œì”©
                result = await db.execute(query)
                tools_without_summary = result.scalars().all()

                if tools_without_summary:
                    print(f"\nğŸ§  AI ìš”ì•½ ìƒì„± ì‹œì‘ ({len(tools_without_summary)}ê°œ ë„êµ¬)...")

                    for tool in tools_without_summary:
                        try:
                            summary_data = await ai_service.summarize_ai_tool(
                                name=tool.tool_name,
                                description=tool.description or "",
                                category=tool.category,
                                use_cases=tool.use_cases or [],
                            )

                            if summary_data.get("summary"):
                                tool.summary = summary_data["summary"]
                                tool.keywords = summary_data.get("keywords", [])
                                print(f"  âœ… {tool.tool_name[:40]} - ìš”ì•½ ì™„ë£Œ")
                            else:
                                print(f"  âš ï¸  {tool.tool_name[:40]} - ìš”ì•½ ì‹¤íŒ¨")

                            # API í˜¸ì¶œ ì œí•œ íšŒí”¼
                            await asyncio.sleep(2)

                        except Exception as e:
                            print(f"  âŒ {tool.tool_name[:40]} - ì—ëŸ¬: {e}")
                            continue

                    await db.commit()
                    print(f"âœ… AI ìš”ì•½ ì™„ë£Œ")
            else:
                print("âš ï¸  Gemini API í‚¤ê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤")

        except Exception as e:
            print(f"âŒ Tool ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            await db.close()

    await _invalidate_cache_after_collection("collect_tools")

    print(f"\n{'='*60}")
    print(f"âœ¨ Tool ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")


async def collect_job_data():
    """AI Job Trend ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…"""
    print(f"\n{'='*60}")
    print(f"ğŸ’¼ AI Job Trend ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    async with AsyncSessionLocal() as db:
        try:
            # 1. RemoteOKì—ì„œ AI/ML ì±„ìš© ê³µê³  ìˆ˜ì§‘
            job_service = JobTrendService()

            jobs = await job_service.fetch_remoteok_jobs(max_results=100)
            if len(jobs) < 100:
                fallback_jobs = await job_service.fetch_sample_jobs()
                jobs.extend(fallback_jobs)
                jobs = jobs[:100]

            if jobs:
                saved = await job_service.save_to_db(jobs, db)
                print(f"âœ… AI Job Trend: {saved}ê°œ ì‹ ê·œ ì±„ìš© ê³µê³  ì €ì¥")
            else:
                print("âš ï¸  ì±„ìš© ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 2. AI ìš”ì•½ ìƒì„± (ìš”ì•½ì´ ì—†ëŠ” ì±„ìš© ê³µê³ ë“¤ì— ëŒ€í•´)
            ai_service = AISummaryService()
            if ai_service.model:  # API í‚¤ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                query = select(AIJobTrend).where(
                    AIJobTrend.summary == None
                ).limit(10)  # í•œë²ˆì— 10ê°œì”©
                result = await db.execute(query)
                jobs_without_summary = result.scalars().all()

                if jobs_without_summary:
                    print(f"\nğŸ§  AI ìš”ì•½ ìƒì„± ì‹œì‘ ({len(jobs_without_summary)}ê°œ ì±„ìš© ê³µê³ )...")

                    for job in jobs_without_summary:
                        try:
                            summary_data = await ai_service.summarize_job(
                                title=job.job_title,
                                company=job.company_name,
                                description=job.description or "",
                                skills=job.required_skills or [],
                            )

                            if summary_data.get("summary"):
                                job.summary = summary_data["summary"]
                                job.keywords = summary_data.get("keywords", [])
                                print(f"  âœ… {job.job_title[:40]} - ìš”ì•½ ì™„ë£Œ")
                            else:
                                print(f"  âš ï¸  {job.job_title[:40]} - ìš”ì•½ ì‹¤íŒ¨")

                            # API í˜¸ì¶œ ì œí•œ íšŒí”¼
                            await asyncio.sleep(2)

                        except Exception as e:
                            print(f"  âŒ {job.job_title[:40]} - ì—ëŸ¬: {e}")
                            continue

                    await db.commit()
                    print(f"âœ… AI ìš”ì•½ ì™„ë£Œ")
            else:
                print("âš ï¸  Gemini API í‚¤ê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤")

        except Exception as e:
            print(f"âŒ Job ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            await db.close()

    await _invalidate_cache_after_collection("collect_jobs")

    print(f"\n{'='*60}")
    print(f"âœ¨ Job ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")


async def collect_policy_data():
    """AI Policy ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…"""
    print(f"\n{'='*60}")
    print(f"âš–ï¸ AI Policy ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    async with AsyncSessionLocal() as db:
        try:
            # 1. RSS í”¼ë“œì—ì„œ AI ì •ì±… ë‰´ìŠ¤ ìˆ˜ì§‘
            policy_service = PolicyService()

            policies = await policy_service.fetch_policy_news(max_results=20)

            if policies:
                saved = await policy_service.save_to_db(policies, db)
                print(f"âœ… AI Policy: {saved}ê°œ ì‹ ê·œ ì •ì±… ì €ì¥")
            else:
                print("âš ï¸  ì •ì±… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 2. AI ìš”ì•½ ìƒì„± (ìš”ì•½ì´ ì—†ëŠ” ì •ì±…ë“¤ì— ëŒ€í•´)
            ai_service = AISummaryService()
            if ai_service.model:  # API í‚¤ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                query = select(AIPolicy).where(
                    AIPolicy.summary == None
                ).limit(10)  # í•œë²ˆì— 10ê°œì”©
                result = await db.execute(query)
                policies_without_summary = result.scalars().all()

                if policies_without_summary:
                    print(f"\nğŸ§  AI ìš”ì•½ ìƒì„± ì‹œì‘ ({len(policies_without_summary)}ê°œ ì •ì±…)...")

                    for policy in policies_without_summary:
                        try:
                            summary_data = await ai_service.summarize_policy(
                                title=policy.title,
                                description=policy.description or "",
                                policy_type=policy.policy_type,
                                impact_areas=policy.impact_areas or [],
                            )

                            if summary_data.get("summary"):
                                policy.summary = summary_data["summary"]
                                policy.keywords = summary_data.get("keywords", [])
                                print(f"  âœ… {policy.title[:40]} - ìš”ì•½ ì™„ë£Œ")
                            else:
                                print(f"  âš ï¸  {policy.title[:40]} - ìš”ì•½ ì‹¤íŒ¨")

                            # API í˜¸ì¶œ ì œí•œ íšŒí”¼
                            await asyncio.sleep(2)

                        except Exception as e:
                            print(f"  âŒ {policy.title[:40]} - ì—ëŸ¬: {e}")
                            continue

                    await db.commit()
                    print(f"âœ… AI ìš”ì•½ ì™„ë£Œ")
            else:
                print("âš ï¸  Gemini API í‚¤ê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤")

        except Exception as e:
            print(f"âŒ Policy ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            await db.close()

    await _invalidate_cache_after_collection("collect_policies")

    print(f"\n{'='*60}")
    print(f"âœ¨ Policy ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")


async def collect_external_trending_keywords():
    """ì™¸ë¶€ íŠ¸ë Œë”© í‚¤ì›Œë“œ ìºì‹œ ê°±ì‹ ."""
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ ì™¸ë¶€ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    try:
        service = ExternalTrendingKeywordService()
        payload = await service.get_keywords(limit=50, force_refresh=True)
        print(
            "âœ… ì™¸ë¶€ íŠ¸ë Œë”© í‚¤ì›Œë“œ ê°±ì‹  ì™„ë£Œ "
            f"(count={len(payload.get('keywords', []))})"
        )
    except Exception as e:
        print(f"âŒ ì™¸ë¶€ íŠ¸ë Œë”© í‚¤ì›Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    await _invalidate_cache_after_collection("collect_external_trending_keywords")


async def collect_all_data():
    """ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…"""
    print(f"\n{'='*80}")
    print(f"ğŸš€ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*80}\n")

    await collect_huggingface_data()
    await asyncio.sleep(3)  # ì ê¹ ëŒ€ê¸°

    await collect_youtube_data()
    await asyncio.sleep(3)  # ì ê¹ ëŒ€ê¸°

    await collect_papers_data()
    await asyncio.sleep(3)  # ì ê¹ ëŒ€ê¸°

    await collect_news_data()
    await asyncio.sleep(3)  # ì ê¹ ëŒ€ê¸°

    await collect_github_data()
    await asyncio.sleep(3)  # ì ê¹ ëŒ€ê¸°

    await collect_conference_data()
    await asyncio.sleep(3)  # ì ê¹ ëŒ€ê¸°

    await collect_tool_data()
    await asyncio.sleep(3)  # ì ê¹ ëŒ€ê¸°

    await collect_job_data()
    await asyncio.sleep(3)  # ì ê¹ ëŒ€ê¸°

    await collect_policy_data()

    # ì™¸ë¶€ íŠ¸ë Œë”© í‚¤ì›Œë“œ ìºì‹œ ê°±ì‹ 
    await collect_external_trending_keywords()

    print(f"\n{'='*80}")
    print(f"ğŸ‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*80}\n")


def start_scheduler():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ - ì¹´í…Œê³ ë¦¬ë³„ ìµœì  ì£¼ê¸° (ChatGPT deep research 2026-02)"""
    logger = logging.getLogger(__name__)

    # â”€â”€ ê³ ë¹ˆë„: ë‰´ìŠ¤ (ë§¤ 1ì‹œê°„) â”€â”€
    scheduler.add_job(
        collect_news_data,
        trigger=CronTrigger(minute=5),  # ë§¤ì‹œ 05ë¶„
        id="collect_news",
        name="AI ë‰´ìŠ¤ ìˆ˜ì§‘ (ë§¤ 1ì‹œê°„)",
        replace_existing=True,
    )

    # â”€â”€ ì¤‘ë¹ˆë„: YouTube (ë§¤ 4ì‹œê°„) â”€â”€
    scheduler.add_job(
        collect_youtube_data,
        trigger=CronTrigger(hour="0,4,8,12,16,20", minute=10),
        id="collect_youtube",
        name="YouTube ìˆ˜ì§‘ (ë§¤ 4ì‹œê°„)",
        replace_existing=True,
    )

    # â”€â”€ ì¤‘ë¹ˆë„: HuggingFace (ë§¤ 6ì‹œê°„) â”€â”€
    scheduler.add_job(
        collect_huggingface_data,
        trigger=CronTrigger(hour="0,6,12,18", minute=15),
        id="collect_huggingface",
        name="HuggingFace ìˆ˜ì§‘ (ë§¤ 6ì‹œê°„)",
        replace_existing=True,
    )

    # â”€â”€ ì¤‘ë¹ˆë„: GitHub (ë§¤ 6ì‹œê°„) â”€â”€
    scheduler.add_job(
        collect_github_data,
        trigger=CronTrigger(hour="1,7,13,19", minute=15),
        id="collect_github",
        name="GitHub ìˆ˜ì§‘ (ë§¤ 6ì‹œê°„)",
        replace_existing=True,
    )

    # â”€â”€ ì¤‘ë¹ˆë„: ì™¸ë¶€ íŠ¸ë Œë”© í‚¤ì›Œë“œ (ë§¤ 6ì‹œê°„) â”€â”€
    scheduler.add_job(
        collect_external_trending_keywords,
        trigger=CronTrigger(hour="0,6,12,18", minute=25),
        id="collect_external_trending_keywords",
        name="ì™¸ë¶€ íŠ¸ë Œë”© í‚¤ì›Œë“œ ìˆ˜ì§‘ (ë§¤ 6ì‹œê°„)",
        replace_existing=True,
    )

    # â”€â”€ ì¤‘ë¹ˆë„: ì±„ìš© (ë§¤ 6ì‹œê°„) â”€â”€
    scheduler.add_job(
        collect_job_data,
        trigger=CronTrigger(hour="2,8,14,20", minute=15),
        id="collect_jobs",
        name="AI ì±„ìš© ìˆ˜ì§‘ (ë§¤ 6ì‹œê°„)",
        replace_existing=True,
    )

    # â”€â”€ ì €ë¹ˆë„: ë…¼ë¬¸ (ë§¤ 12ì‹œê°„) â”€â”€
    scheduler.add_job(
        collect_papers_data,
        trigger=CronTrigger(hour="3,15", minute=0),
        id="collect_papers",
        name="AI ë…¼ë¬¸ ìˆ˜ì§‘ (ë§¤ 12ì‹œê°„)",
        replace_existing=True,
    )

    # â”€â”€ ì¼ê°„: ì»¨í¼ëŸ°ìŠ¤ (ë§¤ì¼ 06:00) â”€â”€
    scheduler.add_job(
        collect_conference_data,
        trigger=CronTrigger(hour=6, minute=0),
        id="collect_conferences",
        name="AI ì»¨í¼ëŸ°ìŠ¤ ìˆ˜ì§‘ (ë§¤ì¼)",
        replace_existing=True,
    )

    # â”€â”€ ì¼ê°„: ì •ì±… (ë§¤ì¼ 07:00) â”€â”€
    scheduler.add_job(
        collect_policy_data,
        trigger=CronTrigger(hour=7, minute=0),
        id="collect_policies",
        name="AI ì •ì±… ìˆ˜ì§‘ (ë§¤ì¼)",
        replace_existing=True,
    )

    # â”€â”€ ì£¼ê°„: í”Œë«í¼/ë„êµ¬ (ë§¤ì£¼ ì›”ìš”ì¼ 04:00) â”€â”€
    scheduler.add_job(
        collect_tool_data,
        trigger=CronTrigger(day_of_week="mon", hour=4, minute=0),
        id="collect_tools",
        name="AI í”Œë«í¼ ìˆ˜ì§‘ (ë§¤ì£¼)",
        replace_existing=True,
    )

    # â”€â”€ ì¼ê°„: 30ì¼ ì´ˆê³¼ ë°ì´í„° ì•„ì¹´ì´ë¸Œ (ë§¤ì¼ 03:30) â”€â”€
    scheduler.add_job(
        archive_old_data,
        trigger=CronTrigger(hour=3, minute=30),
        id="archive_old_data",
        name="ì˜¤ë˜ëœ ë°ì´í„° ì•„ì¹´ì´ë¸Œ (ë§¤ì¼)",
        replace_existing=True,
    )

    scheduler.add_listener(_scheduler_event_listener, EVENT_JOB_EXECUTED | EVENT_JOB_ERROR)

    schedule_info = (
        "â° ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ì¹´í…Œê³ ë¦¬ë³„ ìµœì  ì£¼ê¸°):\n"
        "  - ë‰´ìŠ¤: ë§¤ 1ì‹œê°„ | YouTube: ë§¤ 4ì‹œê°„\n"
        "  - HuggingFace/GitHub/ì±„ìš©/ì™¸ë¶€í‚¤ì›Œë“œ: ë§¤ 6ì‹œê°„\n"
        "  - ë…¼ë¬¸: ë§¤ 12ì‹œê°„ | ì»¨í¼ëŸ°ìŠ¤/ì •ì±…: ë§¤ì¼\n"
        "  - í”Œë«í¼: ë§¤ì£¼ ì›”ìš”ì¼"
    )
    logger.info(schedule_info)
    print(schedule_info)
    scheduler.start()


def stop_scheduler():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€"""
    scheduler.shutdown()
    print("â° ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ")


async def run_collection_now():
    """ì¦‰ì‹œ ìˆ˜ì§‘ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)"""
    await collect_all_data()
