"""ìŠ¤ì¼€ì¤„ëŸ¬ ì„œë¹„ìŠ¤ - ì •ê¸°ì  ë°ì´í„° ìˆ˜ì§‘"""
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.events import EVENT_JOB_EXECUTED, EVENT_JOB_ERROR
from datetime import datetime, timedelta, timezone
import asyncio
import logging
from typing import Any, Awaitable, Callable, TypeVar

from app.config import get_settings
from app.database import AsyncSessionLocal
from app.services.huggingface_service import HuggingFaceService
from app.services.youtube_service import YouTubeService
from app.services.arxiv_service import ArxivService
from app.services.news_service import NewsService
from app.services.github_service import GitHubService
from app.services.conference_service import ConferenceService
from app.services.ai_tool_service import AIToolService
from app.services.job_trend_service import JobTrendService
from app.services.policy_service import PolicyService
from app.services.ai_summary_service import AISummaryService
from app.services.trending_keyword_service import ExternalTrendingKeywordService
from app.models.huggingface import HuggingFaceModel
from app.models.youtube import YouTubeVideo
from app.models.paper import AIPaper
from app.models.news import AINews
from app.models.github import GitHubProject
from app.models.conference import AIConference
from app.models.ai_tool import AITool
from app.models.job_trend import AIJobTrend
from app.models.policy import AIPolicy
from sqlalchemy import select, desc, update, or_
from app.cache import cache_delete_pattern
from app.services.notification_service import send_error_webhook
from app.db_compat import has_columns

settings = get_settings()
logger = logging.getLogger(__name__)


def _log_print(*args, **kwargs):
    """ê¸°ì¡´ print í˜¸ì¶œì„ êµ¬ì¡°í™” ë¡œê¹…ìœ¼ë¡œ ìš°íšŒ."""
    sep = kwargs.get("sep", " ")
    message = sep.join(str(arg) for arg in args)
    logger.info(message)


# NOTE: ëª¨ë“ˆ ë‚´ ëŒ€ëŸ‰ printë¥¼ ë‹¨ê³„ì ìœ¼ë¡œ ì œê±°í•˜ê¸° ì „ê¹Œì§€ loggerë¡œ ë¼ìš°íŒ…
print = _log_print  # type: ignore[assignment]

# ìŠ¤ì¼€ì¤„ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
scheduler = AsyncIOScheduler()

# ì‘ì—…ë³„ ìµœê·¼ ì‹¤í–‰ ìƒíƒœ (System APIì—ì„œ ì‚¬ìš©)
JOB_RUNTIME_STATUS = {}
SummaryRow = TypeVar("SummaryRow")


async def _invalidate_cache_after_collection(job_id: str):
    """ìˆ˜ì§‘ ì™„ë£Œ í›„ ëŒ€ì‹œë³´ë“œ/ê²€ìƒ‰ ìºì‹œ ë¬´íš¨í™”."""
    await cache_delete_pattern("dashboard:*")
    await cache_delete_pattern("search:*")
    await cache_delete_pattern("system:*")
    await cache_delete_pattern("list:*")
    print(f"ğŸ§¹ ìºì‹œ ë¬´íš¨í™” ì™„ë£Œ ({job_id})")


def _scheduler_event_listener(event):
    """APScheduler ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ: ë§ˆì§€ë§‰ ì‹¤í–‰/ì„±ê³µ/ì‹¤íŒ¨ ìƒíƒœ ê¸°ë¡."""
    job_id = getattr(event, "job_id", "unknown")
    now = datetime.now(timezone.utc).isoformat()
    if getattr(event, "exception", None):
        error_message = str(event.exception)
        JOB_RUNTIME_STATUS[job_id] = {
            "last_run": now,
            "last_status": "error",
            "last_error": error_message,
        }
        try:
            loop = asyncio.get_running_loop()
            loop.create_task(
                send_error_webhook(
                    title="Scheduler Job Failed",
                    job_id=job_id,
                    message=error_message,
                )
            )
        except RuntimeError:
            pass
    else:
        JOB_RUNTIME_STATUS[job_id] = {
            "last_run": now,
            "last_status": "success",
            "last_error": None,
        }
        try:
            loop = asyncio.get_running_loop()
            loop.create_task(_invalidate_cache_after_collection(job_id))
        except RuntimeError:
            pass


def get_scheduler_runtime_status():
    """ì‘ì—…ë³„ ëŸ°íƒ€ì„ ìƒíƒœ ë°˜í™˜."""
    return JOB_RUNTIME_STATUS


async def archive_old_data(days: int = 30):
    """30ì¼ ì´ˆê³¼ ë°ì´í„° soft archive."""
    cutoff = datetime.now(timezone.utc) - timedelta(days=days)
    archived_at = datetime.now(timezone.utc)
    print(
        f"\nğŸ—„ï¸  ì•„ì¹´ì´ë¸Œ ì‘ì—… ì‹œì‘ (cutoff={cutoff.strftime('%Y-%m-%d %H:%M:%S')} UTC)"
    )

    archived_summary = {}
    model_specs = [
        ("huggingface", "huggingface_models", HuggingFaceModel, HuggingFaceModel.collected_at),
        ("youtube", "youtube_videos", YouTubeVideo, YouTubeVideo.published_at),
        ("papers", "ai_papers", AIPaper, AIPaper.published_date),
        ("news", "ai_news", AINews, AINews.published_date),
        ("github", "github_projects", GitHubProject, GitHubProject.pushed_at),
        ("conferences", "ai_conferences", AIConference, AIConference.end_date),
        ("tools", "ai_tools", AITool, AITool.created_at),
        ("jobs", "ai_job_trends", AIJobTrend, AIJobTrend.posted_date),
        ("policies", "ai_policies", AIPolicy, AIPolicy.effective_date),
    ]

    async with AsyncSessionLocal() as db:
        try:
            for name, table_name, model, date_col in model_specs:
                column_flags = await has_columns(
                    db,
                    table_name,
                    ["is_archived", "archived_at"],
                )
                has_archive_columns = (
                    column_flags["is_archived"] and column_flags["archived_at"]
                )
                if not has_archive_columns:
                    archived_summary[name] = 0
                    continue

                value_payload = {
                    "is_archived": True,
                    "archived_at": archived_at,
                }
                if hasattr(model, "is_trending"):
                    value_payload["is_trending"] = False

                stmt = (
                    update(model)
                    .where(
                        date_col.isnot(None),
                        date_col < cutoff,
                        model.is_archived.is_(False),
                    )
                    .values(**value_payload)
                )
                result = await db.execute(stmt)
                archived_summary[name] = result.rowcount or 0

            await db.commit()
        except Exception as e:
            await db.rollback()
            print(f"âŒ ì•„ì¹´ì´ë¸Œ ì‘ì—… ì‹¤íŒ¨: {e}")
            raise

    total_archived = sum(archived_summary.values())
    details = ", ".join(f"{k}={v}" for k, v in archived_summary.items())
    print(f"âœ… ì•„ì¹´ì´ë¸Œ ì™„ë£Œ: total={total_archived} ({details})")
    await _invalidate_cache_after_collection("archive_old_data")


async def _fill_missing_summaries(
    *,
    db,
    model,
    label: str,
    item_name: Callable[[Any], str],
    build_summary: Callable[[AISummaryService, Any], Awaitable[dict[str, Any]]],
    apply_summary: Callable[[Any, dict[str, Any]], bool],
    limit: int = 10,
) -> int:
    """ì¹´í…Œê³ ë¦¬ë³„ ê³µí†µ ìš”ì•½ ë°±í•„ ë£¨í‹´."""
    ai_service = AISummaryService()
    can_summarize = await ai_service.can_summarize()
    if not can_summarize:
        print("âš ï¸  ì‚¬ìš© ê°€ëŠ¥í•œ ìš”ì•½ provider(Gemini/Ollama)ê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤")
        return 0

    query = select(model).where(or_(model.summary.is_(None), model.summary == "")).limit(limit)
    result = await db.execute(query)
    rows = result.scalars().all()
    if not rows:
        return 0

    print(f"\nğŸ§  AI ìš”ì•½ ìƒì„± ì‹œì‘ ({len(rows)}ê°œ {label})...")
    delay_seconds = 2.0 if ai_service.model is not None else 0.0
    updated = 0

    for row in rows:
        row_title = item_name(row)[:40]
        try:
            summary_data = await build_summary(ai_service, row)
            if apply_summary(row, summary_data):
                updated += 1
                print(f"  âœ… {row_title} - ìš”ì•½ ì™„ë£Œ")
            else:
                print(f"  âš ï¸  {row_title} - ìš”ì•½ ì‹¤íŒ¨")
        except Exception as e:
            print(f"  âŒ {row_title} - ì—ëŸ¬: {e}")
            continue

        if delay_seconds > 0:
            await asyncio.sleep(delay_seconds)

    if updated > 0:
        await db.commit()
        print("âœ… AI ìš”ì•½ ì™„ë£Œ")

    return updated


async def collect_huggingface_data():
    """Hugging Face ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…"""
    print(f"\n{'='*60}")
    print(f"ğŸ¤– ìë™ ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    async with AsyncSessionLocal() as db:
        try:
            # 1. íŠ¸ë Œë”© ëª¨ë¸ ìˆ˜ì§‘
            hf_service = HuggingFaceService()
            result = await hf_service.collect_trending_models(db, limit=50)

            if result["success"]:
                print(f"âœ… Hugging Face: {result['count']}ê°œ ì‹ ê·œ ëª¨ë¸ ì €ì¥")
            else:
                print("âš ï¸  Hugging Face ìˆ˜ì§‘ ì‹¤íŒ¨")

            # 2. AI ìš”ì•½ ìƒì„± (ìš”ì•½ì´ ì—†ëŠ” ëª¨ë¸ë“¤ì— ëŒ€í•´)
            def _apply_hf_summary(row: HuggingFaceModel, payload: dict[str, Any]) -> bool:
                if not payload.get("summary"):
                    return False
                row.summary = payload.get("summary")
                row.key_features = payload.get("key_features", [])
                row.use_cases = payload.get("use_cases")
                return True

            await _fill_missing_summaries(
                db=db,
                model=HuggingFaceModel,
                label="ëª¨ë¸",
                item_name=lambda row: row.model_name or row.model_id or "unknown",
                build_summary=lambda ai, row: ai.summarize_huggingface_model(
                    model_name=row.model_name,
                    description=row.description,
                    task=row.task,
                    tags=row.tags or [],
                ),
                apply_summary=_apply_hf_summary,
                limit=10,
            )

        except Exception as e:
            print(f"âŒ ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            await db.close()

    await _invalidate_cache_after_collection("collect_huggingface")

    print(f"\n{'='*60}")
    print(f"âœ¨ ìë™ ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")


async def collect_youtube_data():
    """YouTube ë°ì´í„° ìˆ˜ì§‘ ì‘ì—… (íë ˆì´ì…˜ ì±„ë„ + í‚¤ì›Œë“œ ê²€ìƒ‰)"""
    print(f"\n{'='*60}")
    print(f"ğŸ“º YouTube ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    async with AsyncSessionLocal() as db:
        try:
            from app.models.youtube_channel import YouTubeChannel

            yt_service = YouTubeService()
            total_saved = 0

            # 1. íë ˆì´ì…˜ëœ ì±„ë„ì˜ ìµœì‹  ì˜ìƒ ìˆ˜ì§‘ (ìš°ì„ ìˆœìœ„ ë†’ì€ ìˆœ)
            print("ğŸ“Œ íë ˆì´ì…˜ëœ AI ìœ íŠœë²„ ì±„ë„ì—ì„œ ìµœì‹  ì˜ìƒ ìˆ˜ì§‘ ì¤‘...")
            result = await db.execute(
                select(YouTubeChannel)
                .where(YouTubeChannel.is_active == True)
                .order_by(desc(YouTubeChannel.priority))
                .limit(30)  # ìµœëŒ€ 30ê°œ ì±„ë„
            )
            channels = result.scalars().all()

            channel_videos_count = 0
            for channel in channels:
                try:
                    category_text = (channel.category or "").lower()
                    is_korean_channel = any(
                        token in category_text
                        for token in ["êµ­ë‚´", "korean", "ko"]
                    )
                    # v4.0: YouTubeëŠ” êµ­ë‚´ ì±„ë„ë§Œ ìˆ˜ì§‘
                    if not is_korean_channel:
                        continue

                    channel_lang = "ko"
                    videos = await yt_service.get_channel_videos(
                        channel_id=channel.channel_id,
                        max_results=15,  # ì±„ë„ë‹¹ ìµœì‹  15ê°œ
                        order="date",
                        relevance_language=channel_lang,
                        default_language=channel_lang,
                    )

                    if videos:
                        saved = await yt_service.save_videos_to_db(videos, db)
                        channel_videos_count += saved
                        if saved > 0:
                            print(
                                f"  âœ… {channel.channel_name}: {saved}ê°œ ì‹ ê·œ ì˜ìƒ"
                            )

                        # ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì‹œê°„ ì—…ë°ì´íŠ¸
                        channel.last_collected_at = datetime.now()
                        await db.commit()

                    await asyncio.sleep(0.5)  # API í˜¸ì¶œ ì œí•œ íšŒí”¼

                except Exception as e:
                    print(f"  âŒ {channel.channel_name}: {e}")
                    continue

            print(
                f"âœ… íë ˆì´ì…˜ ì±„ë„: {channel_videos_count}ê°œ ì‹ ê·œ ì˜ìƒ ì €ì¥\n"
            )

            # 2. í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ì¶”ê°€ AI íŠ¸ë Œë“œ ì˜ìƒ ìˆ˜ì§‘
            print("ğŸ“Œ í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ì¶”ê°€ AI íŠ¸ë Œë“œ ì˜ìƒ ìˆ˜ì§‘ ì¤‘...")
            queries = [
                ("ì¸ê³µì§€ëŠ¥ ê°œë°œ íŠœí† ë¦¬ì–¼", "ko"),
                ("LLM ì‹¤ë¬´ í™œìš©", "ko"),
                ("ë¨¸ì‹ ëŸ¬ë‹ ì…ë¬¸", "ko"),
                ("ë”¥ëŸ¬ë‹ ìµœì‹  ë™í–¥", "ko"),
                ("ì±—GPT í™œìš©ë²•", "ko"),
                ("RAG í”„ë¡œì íŠ¸", "ko"),
            ]

            keyword_videos_count = 0
            for query, lang in queries:
                videos = await yt_service.search_ai_videos(
                    query=query,
                    max_results=15,
                    order="viewCount",
                    relevance_language=lang,
                )

                if videos:
                    saved = await yt_service.save_videos_to_db(videos, db)
                    keyword_videos_count += saved
                    if saved > 0:
                        print(f"  âœ… '{query}': {saved}ê°œ ì‹ ê·œ ë¹„ë””ì˜¤")

                await asyncio.sleep(1)  # API í˜¸ì¶œ ì œí•œ íšŒí”¼

            print(f"âœ… í‚¤ì›Œë“œ ê²€ìƒ‰: {keyword_videos_count}ê°œ ì‹ ê·œ ì˜ìƒ ì €ì¥")

            total_saved = channel_videos_count + keyword_videos_count
            print(f"\nâœ… YouTube ì „ì²´: ì´ {total_saved}ê°œ ì‹ ê·œ ë¹„ë””ì˜¤ ì €ì¥")

            # 2. AI ìš”ì•½ ìƒì„± (ìš”ì•½ì´ ì—†ëŠ” ë¹„ë””ì˜¤ë“¤ì— ëŒ€í•´)
            def _apply_youtube_summary(row: YouTubeVideo, payload: dict[str, Any]) -> bool:
                if not payload.get("summary"):
                    return False
                row.summary = payload.get("summary")
                row.keywords = payload.get("keywords", [])
                row.key_points = payload.get("key_points", [])
                return True

            await _fill_missing_summaries(
                db=db,
                model=YouTubeVideo,
                label="ë¹„ë””ì˜¤",
                item_name=lambda row: row.title or row.video_id or "unknown",
                build_summary=lambda ai, row: ai.summarize_youtube_video(
                    title=row.title,
                    description=row.description,
                    tags=row.tags or [],
                ),
                apply_summary=_apply_youtube_summary,
                limit=10,
            )

        except Exception as e:
            print(f"âŒ YouTube ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            await db.close()

    await _invalidate_cache_after_collection("collect_youtube")

    print(f"\n{'='*60}")
    print(f"âœ¨ YouTube ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")


async def collect_papers_data():
    """AI Papers ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…"""
    print(f"\n{'='*60}")
    print(f"ğŸ“„ AI Papers ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    async with AsyncSessionLocal() as db:
        try:
            # 1. arXivì—ì„œ ìµœê·¼ ë…¼ë¬¸ ê²€ìƒ‰
            arxiv_service = ArxivService()

            # ìµœê·¼ 7ì¼ê°„ì˜ AI ë…¼ë¬¸ ìˆ˜ì§‘
            papers = await arxiv_service.search_recent_papers(
                days=7, max_results=100
            )

            if papers:
                saved = await arxiv_service.save_papers_to_db(papers, db)
                print(f"âœ… arXiv: {saved}ê°œ ì‹ ê·œ ë…¼ë¬¸ ì €ì¥")
            else:
                print("âš ï¸  arXivì—ì„œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 2. AI ìš”ì•½ ìƒì„± (ìš”ì•½ì´ ì—†ëŠ” ë…¼ë¬¸ë“¤ì— ëŒ€í•´)
            def _apply_paper_summary(row: AIPaper, payload: dict[str, Any]) -> bool:
                if not payload.get("summary"):
                    return False
                row.summary = payload.get("summary")
                row.keywords = payload.get("keywords", [])
                row.key_contributions = payload.get("key_contributions", [])
                return True

            await _fill_missing_summaries(
                db=db,
                model=AIPaper,
                label="ë…¼ë¬¸",
                item_name=lambda row: row.title or row.arxiv_id or "unknown",
                build_summary=lambda ai, row: ai.summarize_paper(
                    title=row.title,
                    abstract=row.abstract,
                    authors=row.authors or [],
                    categories=row.categories or [],
                ),
                apply_summary=_apply_paper_summary,
                limit=10,
            )

        except Exception as e:
            print(f"âŒ Papers ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            await db.close()

    await _invalidate_cache_after_collection("collect_papers")

    print(f"\n{'='*60}")
    print(f"âœ¨ Papers ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")


async def collect_news_data():
    """AI News ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…"""
    print(f"\n{'='*60}")
    print(f"ğŸ“° AI News ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    async with AsyncSessionLocal() as db:
        try:
            # 1. RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
            news_service = NewsService()

            articles = await news_service.fetch_all_feeds()

            if articles:
                saved = await news_service.save_news_to_db(articles, db)
                print(f"\nâœ… AI News: ì´ {saved}ê°œ ì‹ ê·œ ë‰´ìŠ¤ ì €ì¥")
            else:
                print("âš ï¸  RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 2. AI ìš”ì•½ ìƒì„± (ìš”ì•½ì´ ì—†ëŠ” ë‰´ìŠ¤ë“¤ì— ëŒ€í•´)
            def _apply_news_summary(row: AINews, payload: dict[str, Any]) -> bool:
                if not payload.get("summary"):
                    return False
                row.summary = payload.get("summary")
                row.keywords = payload.get("keywords", [])
                row.key_points = payload.get("key_points", [])
                return True

            await _fill_missing_summaries(
                db=db,
                model=AINews,
                label="ë‰´ìŠ¤",
                item_name=lambda row: row.title or row.url or "unknown",
                build_summary=lambda ai, row: ai.summarize_news(
                    title=row.title,
                    content=row.content or row.excerpt,
                    source=row.source,
                ),
                apply_summary=_apply_news_summary,
                limit=10,
            )

        except Exception as e:
            print(f"âŒ News ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            await db.close()

    await _invalidate_cache_after_collection("collect_news")

    print(f"\n{'='*60}")
    print(f"âœ¨ News ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")


async def collect_github_data():
    """GitHub íŠ¸ë Œë”© í”„ë¡œì íŠ¸ ìˆ˜ì§‘ ì‘ì—…"""
    print(f"\n{'='*60}")
    print(f"â­ GitHub íŠ¸ë Œë”© ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    async with AsyncSessionLocal() as db:
        try:
            # 1. GitHubì—ì„œ íŠ¸ë Œë”© AI/ML í”„ë¡œì íŠ¸ ê²€ìƒ‰
            github_service = GitHubService()

            projects = await github_service.fetch_trending_repos(
                language="", max_results=50
            )

            if projects:
                saved = await github_service.save_projects_to_db(projects, db)
                print(f"âœ… GitHub: {saved}ê°œ ì‹ ê·œ í”„ë¡œì íŠ¸ ì €ì¥")
            else:
                print("âš ï¸  GitHubì—ì„œ í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 2. AI ìš”ì•½ ìƒì„± (ìš”ì•½ì´ ì—†ëŠ” í”„ë¡œì íŠ¸ë“¤ì— ëŒ€í•´)
            def _apply_github_summary(row: GitHubProject, payload: dict[str, Any]) -> bool:
                if not payload.get("summary"):
                    return False
                row.summary = payload.get("summary")
                row.keywords = payload.get("keywords", [])
                row.use_cases = payload.get("use_cases", [])
                return True

            await _fill_missing_summaries(
                db=db,
                model=GitHubProject,
                label="í”„ë¡œì íŠ¸",
                item_name=lambda row: row.repo_name or row.name or "unknown",
                build_summary=lambda ai, row: ai.summarize_github_project(
                    repo_name=row.repo_name,
                    description=row.description,
                    language=row.language,
                    topics=row.topics or [],
                ),
                apply_summary=_apply_github_summary,
                limit=10,
            )

        except Exception as e:
            print(f"âŒ GitHub ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            await db.close()

    await _invalidate_cache_after_collection("collect_github")

    print(f"\n{'='*60}")
    print(f"âœ¨ GitHub ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")


async def collect_conference_data():
    """AI Conference ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…"""
    print(f"\n{'='*60}")
    print(f"ğŸ“… AI Conference ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    async with AsyncSessionLocal() as db:
        try:
            # 1. WikiCFPì—ì„œ AI ì»¨í¼ëŸ°ìŠ¤ ìˆ˜ì§‘
            conference_service = ConferenceService()

            conferences = await conference_service.fetch_wikicfp_conferences(max_results=50)

            if conferences:
                saved = await conference_service.save_to_db(conferences, db)
                print(f"âœ… AI Conference: {saved}ê°œ ì‹ ê·œ ì»¨í¼ëŸ°ìŠ¤ ì €ì¥")
            else:
                print("âš ï¸  WikiCFPì—ì„œ ì»¨í¼ëŸ°ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 2. AI ìš”ì•½ ìƒì„± (ìš”ì•½ì´ ì—†ëŠ” ì»¨í¼ëŸ°ìŠ¤ë“¤ì— ëŒ€í•´)
            def _apply_conference_summary(row: AIConference, payload: dict[str, Any]) -> bool:
                if not payload.get("summary"):
                    return False
                row.summary = payload.get("summary")
                row.keywords = payload.get("keywords", [])
                return True

            await _fill_missing_summaries(
                db=db,
                model=AIConference,
                label="ì»¨í¼ëŸ°ìŠ¤",
                item_name=lambda row: row.conference_name or "unknown",
                build_summary=lambda ai, row: ai.summarize_conference(
                    name=row.conference_name,
                    description=row.summary or "",
                    topics=row.topics or [],
                ),
                apply_summary=_apply_conference_summary,
                limit=10,
            )

        except Exception as e:
            print(f"âŒ Conference ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            await db.close()

    await _invalidate_cache_after_collection("collect_conferences")

    print(f"\n{'='*60}")
    print(f"âœ¨ Conference ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")


async def collect_tool_data():
    """AI Tool ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…"""
    print(f"\n{'='*60}")
    print(f"ğŸ› ï¸ AI Tool ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    async with AsyncSessionLocal() as db:
        try:
            # 1. íŠ¸ë Œë”© AI ë„êµ¬ ìˆ˜ì§‘
            tool_service = AIToolService()

            tools = await tool_service.fetch_trending_tools(max_results=30)

            if tools:
                saved = await tool_service.save_to_db(tools, db)
                print(f"âœ… AI Tool: {saved}ê°œ ì‹ ê·œ ë„êµ¬ ì €ì¥")
            else:
                print("âš ï¸  AI ë„êµ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 2. AI ìš”ì•½ ìƒì„± (ìš”ì•½ì´ ì—†ëŠ” ë„êµ¬ë“¤ì— ëŒ€í•´)
            def _apply_tool_summary(row: AITool, payload: dict[str, Any]) -> bool:
                if not payload.get("summary"):
                    return False
                row.summary = payload.get("summary")
                row.keywords = payload.get("keywords", [])
                return True

            await _fill_missing_summaries(
                db=db,
                model=AITool,
                label="ë„êµ¬",
                item_name=lambda row: row.tool_name or "unknown",
                build_summary=lambda ai, row: ai.summarize_ai_tool(
                    name=row.tool_name,
                    description=row.description or "",
                    category=row.category,
                    use_cases=row.use_cases or [],
                ),
                apply_summary=_apply_tool_summary,
                limit=10,
            )

        except Exception as e:
            print(f"âŒ Tool ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            await db.close()

    await _invalidate_cache_after_collection("collect_tools")

    print(f"\n{'='*60}")
    print(f"âœ¨ Tool ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")


async def collect_job_data():
    """AI Job Trend ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…"""
    print(f"\n{'='*60}")
    print(f"ğŸ’¼ AI Job Trend ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    async with AsyncSessionLocal() as db:
        try:
            # 1. RemoteOKì—ì„œ AI/ML ì±„ìš© ê³µê³  ìˆ˜ì§‘
            job_service = JobTrendService()

            jobs = await job_service.fetch_remoteok_jobs(max_results=100)
            if len(jobs) < 100:
                fallback_jobs = await job_service.fetch_sample_jobs()
                jobs.extend(fallback_jobs)
                jobs = jobs[:100]

            if jobs:
                saved = await job_service.save_to_db(jobs, db)
                print(f"âœ… AI Job Trend: {saved}ê°œ ì‹ ê·œ ì±„ìš© ê³µê³  ì €ì¥")
            else:
                print("âš ï¸  ì±„ìš© ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 2. AI ìš”ì•½ ìƒì„± (ìš”ì•½ì´ ì—†ëŠ” ì±„ìš© ê³µê³ ë“¤ì— ëŒ€í•´)
            def _apply_job_summary(row: AIJobTrend, payload: dict[str, Any]) -> bool:
                if not payload.get("summary"):
                    return False
                row.summary = payload.get("summary")
                row.keywords = payload.get("keywords", [])
                return True

            await _fill_missing_summaries(
                db=db,
                model=AIJobTrend,
                label="ì±„ìš© ê³µê³ ",
                item_name=lambda row: row.job_title or row.job_url or "unknown",
                build_summary=lambda ai, row: ai.summarize_job(
                    title=row.job_title,
                    company=row.company_name,
                    description=row.description or "",
                    skills=row.required_skills or [],
                ),
                apply_summary=_apply_job_summary,
                limit=10,
            )

        except Exception as e:
            print(f"âŒ Job ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            await db.close()

    await _invalidate_cache_after_collection("collect_jobs")

    print(f"\n{'='*60}")
    print(f"âœ¨ Job ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")


async def collect_policy_data():
    """AI Policy ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…"""
    print(f"\n{'='*60}")
    print(f"âš–ï¸ AI Policy ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    async with AsyncSessionLocal() as db:
        try:
            # 1. RSS í”¼ë“œì—ì„œ AI ì •ì±… ë‰´ìŠ¤ ìˆ˜ì§‘
            policy_service = PolicyService()

            policies = await policy_service.fetch_policy_news(max_results=20)

            if policies:
                saved = await policy_service.save_to_db(policies, db)
                print(f"âœ… AI Policy: {saved}ê°œ ì‹ ê·œ ì •ì±… ì €ì¥")
            else:
                print("âš ï¸  ì •ì±… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 2. AI ìš”ì•½ ìƒì„± (ìš”ì•½ì´ ì—†ëŠ” ì •ì±…ë“¤ì— ëŒ€í•´)
            def _apply_policy_summary(row: AIPolicy, payload: dict[str, Any]) -> bool:
                if not payload.get("summary"):
                    return False
                row.summary = payload.get("summary")
                row.keywords = payload.get("keywords", [])
                return True

            await _fill_missing_summaries(
                db=db,
                model=AIPolicy,
                label="ì •ì±…",
                item_name=lambda row: row.title or row.source_url or "unknown",
                build_summary=lambda ai, row: ai.summarize_policy(
                    title=row.title,
                    description=row.description or "",
                    policy_type=row.policy_type,
                    impact_areas=row.impact_areas or [],
                ),
                apply_summary=_apply_policy_summary,
                limit=10,
            )

        except Exception as e:
            print(f"âŒ Policy ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            await db.close()

    await _invalidate_cache_after_collection("collect_policies")

    print(f"\n{'='*60}")
    print(f"âœ¨ Policy ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")


async def collect_external_trending_keywords():
    """ì™¸ë¶€ íŠ¸ë Œë”© í‚¤ì›Œë“œ ìºì‹œ ê°±ì‹ ."""
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ ì™¸ë¶€ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    try:
        service = ExternalTrendingKeywordService()
        payload = await service.get_keywords(limit=50, force_refresh=True)
        print(
            "âœ… ì™¸ë¶€ íŠ¸ë Œë”© í‚¤ì›Œë“œ ê°±ì‹  ì™„ë£Œ "
            f"(count={len(payload.get('keywords', []))})"
        )
    except Exception as e:
        print(f"âŒ ì™¸ë¶€ íŠ¸ë Œë”© í‚¤ì›Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    await _invalidate_cache_after_collection("collect_external_trending_keywords")


async def collect_all_data():
    """ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘ ì‘ì—…"""
    print(f"\n{'='*80}")
    print(f"ğŸš€ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*80}\n")

    await collect_huggingface_data()
    await asyncio.sleep(3)  # ì ê¹ ëŒ€ê¸°

    await collect_youtube_data()
    await asyncio.sleep(3)  # ì ê¹ ëŒ€ê¸°

    await collect_papers_data()
    await asyncio.sleep(3)  # ì ê¹ ëŒ€ê¸°

    await collect_news_data()
    await asyncio.sleep(3)  # ì ê¹ ëŒ€ê¸°

    await collect_github_data()
    await asyncio.sleep(3)  # ì ê¹ ëŒ€ê¸°

    await collect_conference_data()
    await asyncio.sleep(3)  # ì ê¹ ëŒ€ê¸°

    await collect_tool_data()
    await asyncio.sleep(3)  # ì ê¹ ëŒ€ê¸°

    await collect_job_data()
    await asyncio.sleep(3)  # ì ê¹ ëŒ€ê¸°

    await collect_policy_data()

    # ì™¸ë¶€ íŠ¸ë Œë”© í‚¤ì›Œë“œ ìºì‹œ ê°±ì‹ 
    await collect_external_trending_keywords()

    print(f"\n{'='*80}")
    print(f"ğŸ‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*80}\n")


def start_scheduler():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ - ì¹´í…Œê³ ë¦¬ë³„ ìµœì  ì£¼ê¸° (ChatGPT deep research 2026-02)"""
    logger = logging.getLogger(__name__)
    job_configs = [
        # â”€â”€ ê³ ë¹ˆë„: ë‰´ìŠ¤ (ë§¤ 1ì‹œê°„) â”€â”€
        {
            "func": collect_news_data,
            "trigger": CronTrigger(minute=5),  # ë§¤ì‹œ 05ë¶„
            "id": "collect_news",
            "name": "AI ë‰´ìŠ¤ ìˆ˜ì§‘ (ë§¤ 1ì‹œê°„)",
        },
        # â”€â”€ ì¤‘ë¹ˆë„: YouTube (ë§¤ 4ì‹œê°„) â”€â”€
        {
            "func": collect_youtube_data,
            "trigger": CronTrigger(hour="0,4,8,12,16,20", minute=10),
            "id": "collect_youtube",
            "name": "YouTube ìˆ˜ì§‘ (ë§¤ 4ì‹œê°„)",
        },
        # â”€â”€ ì¤‘ë¹ˆë„: HuggingFace (ë§¤ 6ì‹œê°„) â”€â”€
        {
            "func": collect_huggingface_data,
            "trigger": CronTrigger(hour="0,6,12,18", minute=15),
            "id": "collect_huggingface",
            "name": "HuggingFace ìˆ˜ì§‘ (ë§¤ 6ì‹œê°„)",
        },
        # â”€â”€ ì¤‘ë¹ˆë„: GitHub (ë§¤ 6ì‹œê°„) â”€â”€
        {
            "func": collect_github_data,
            "trigger": CronTrigger(hour="1,7,13,19", minute=15),
            "id": "collect_github",
            "name": "GitHub ìˆ˜ì§‘ (ë§¤ 6ì‹œê°„)",
        },
        # â”€â”€ ì¤‘ë¹ˆë„: ì™¸ë¶€ íŠ¸ë Œë”© í‚¤ì›Œë“œ (ë§¤ 6ì‹œê°„) â”€â”€
        {
            "func": collect_external_trending_keywords,
            "trigger": CronTrigger(hour="0,6,12,18", minute=25),
            "id": "collect_external_trending_keywords",
            "name": "ì™¸ë¶€ íŠ¸ë Œë”© í‚¤ì›Œë“œ ìˆ˜ì§‘ (ë§¤ 6ì‹œê°„)",
        },
        # â”€â”€ ì¤‘ë¹ˆë„: ì±„ìš© (ë§¤ 6ì‹œê°„) â”€â”€
        {
            "func": collect_job_data,
            "trigger": CronTrigger(hour="2,8,14,20", minute=15),
            "id": "collect_jobs",
            "name": "AI ì±„ìš© ìˆ˜ì§‘ (ë§¤ 6ì‹œê°„)",
        },
        # â”€â”€ ì €ë¹ˆë„: ë…¼ë¬¸ (ë§¤ 12ì‹œê°„) â”€â”€
        {
            "func": collect_papers_data,
            "trigger": CronTrigger(hour="3,15", minute=0),
            "id": "collect_papers",
            "name": "AI ë…¼ë¬¸ ìˆ˜ì§‘ (ë§¤ 12ì‹œê°„)",
        },
        # â”€â”€ ì¼ê°„: ì»¨í¼ëŸ°ìŠ¤ (ë§¤ì¼ 06:00) â”€â”€
        {
            "func": collect_conference_data,
            "trigger": CronTrigger(hour=6, minute=0),
            "id": "collect_conferences",
            "name": "AI ì»¨í¼ëŸ°ìŠ¤ ìˆ˜ì§‘ (ë§¤ì¼)",
        },
        # â”€â”€ ì¼ê°„: ì •ì±… (ë§¤ì¼ 07:00) â”€â”€
        {
            "func": collect_policy_data,
            "trigger": CronTrigger(hour=7, minute=0),
            "id": "collect_policies",
            "name": "AI ì •ì±… ìˆ˜ì§‘ (ë§¤ì¼)",
        },
        # â”€â”€ ì£¼ê°„: í”Œë«í¼/ë„êµ¬ (ë§¤ì£¼ ì›”ìš”ì¼ 04:00) â”€â”€
        {
            "func": collect_tool_data,
            "trigger": CronTrigger(day_of_week="mon", hour=4, minute=0),
            "id": "collect_tools",
            "name": "AI í”Œë«í¼ ìˆ˜ì§‘ (ë§¤ì£¼)",
        },
        # â”€â”€ ì¼ê°„: 30ì¼ ì´ˆê³¼ ë°ì´í„° ì•„ì¹´ì´ë¸Œ (ë§¤ì¼ 03:30) â”€â”€
        {
            "func": archive_old_data,
            "trigger": CronTrigger(hour=3, minute=30),
            "id": "archive_old_data",
            "name": "ì˜¤ë˜ëœ ë°ì´í„° ì•„ì¹´ì´ë¸Œ (ë§¤ì¼)",
        },
    ]

    for config in job_configs:
        scheduler.add_job(
            config["func"],
            trigger=config["trigger"],
            id=config["id"],
            name=config["name"],
            replace_existing=True,
        )

    scheduler.add_listener(_scheduler_event_listener, EVENT_JOB_EXECUTED | EVENT_JOB_ERROR)

    schedule_info = (
        "â° ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ì¹´í…Œê³ ë¦¬ë³„ ìµœì  ì£¼ê¸°):\n"
        "  - ë‰´ìŠ¤: ë§¤ 1ì‹œê°„ | YouTube: ë§¤ 4ì‹œê°„\n"
        "  - HuggingFace/GitHub/ì±„ìš©/ì™¸ë¶€í‚¤ì›Œë“œ: ë§¤ 6ì‹œê°„\n"
        "  - ë…¼ë¬¸: ë§¤ 12ì‹œê°„ | ì»¨í¼ëŸ°ìŠ¤/ì •ì±…: ë§¤ì¼\n"
        "  - í”Œë«í¼: ë§¤ì£¼ ì›”ìš”ì¼"
    )
    logger.info(schedule_info)
    print(schedule_info)
    scheduler.start()


def stop_scheduler():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€"""
    scheduler.shutdown()
    print("â° ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ")


async def run_collection_now():
    """ì¦‰ì‹œ ìˆ˜ì§‘ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)"""
    await collect_all_data()
