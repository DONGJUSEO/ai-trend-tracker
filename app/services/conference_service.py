"""AI Conference ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤"""
import httpx
import feedparser
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, desc

from app.models.conference import AIConference
from app.config import get_settings


class ConferenceService:
    """AI Conference ë°ì´í„° ìˆ˜ì§‘ ë° ê´€ë¦¬"""

    # WikiCFP RSS feeds for AI-related conferences
    RSS_FEEDS = {
        "ai": "http://www.wikicfp.com/cfp/rss?cat=intelligence&t=c",
        "ml": "http://www.wikicfp.com/cfp/rss?cat=machine+learning&t=c",
        "cv": "http://www.wikicfp.com/cfp/rss?cat=computer+vision&t=c",
        "nlp": "http://www.wikicfp.com/cfp/rss?cat=natural+language&t=c",
    }

    # Well-known AI conferences
    MAJOR_CONFERENCES = {
        "NeurIPS": "A*",
        "ICML": "A*",
        "ICLR": "A*",
        "CVPR": "A*",
        "AAAI": "A*",
        "IJCAI": "A*",
        "ACL": "A*",
        "EMNLP": "A",
        "NAACL": "A",
        "ECCV": "A",
        "ICCV": "A*",
    }

    def __init__(self):
        pass

    async def fetch_wikicfp_conferences(self, max_results: int = 50) -> List[Dict[str, Any]]:
        """
        WikiCFP RSS í”¼ë“œì—ì„œ AI ì»¨í¼ëŸ°ìŠ¤ ì •ë³´ ìˆ˜ì§‘

        Args:
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜

        Returns:
            ì»¨í¼ëŸ°ìŠ¤ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        all_conferences = []

        try:
            for category, feed_url in self.RSS_FEEDS.items():
                print(f"ğŸ“¡ Fetching conferences from WikiCFP ({category})...")

                # feedparserëŠ” ë™ê¸° ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë¯€ë¡œ ì§ì ‘ ì‚¬ìš©
                feed = feedparser.parse(feed_url)

                for entry in feed.entries[:max_results]:
                    conference_data = self._parse_wikicfp_entry(entry, category)
                    if conference_data:
                        all_conferences.append(conference_data)

                print(f"âœ… Fetched {len(feed.entries[:max_results])} entries from {category}")

        except Exception as e:
            print(f"âŒ Error fetching WikiCFP conferences: {e}")

        return all_conferences[:max_results]

    def _parse_wikicfp_entry(self, entry: Any, category: str) -> Optional[Dict[str, Any]]:
        """
        WikiCFP RSS entry íŒŒì‹±

        Args:
            entry: feedparser entry ê°ì²´
            category: ì¹´í…Œê³ ë¦¬ (ai, ml, cv, nlp)

        Returns:
            íŒŒì‹±ëœ ì»¨í¼ëŸ°ìŠ¤ ì •ë³´
        """
        try:
            title = entry.get("title", "")
            link = entry.get("link", "")
            description = entry.get("summary", "")

            # ì œëª©ì—ì„œ ì»¨í¼ëŸ°ìŠ¤ ì•½ì–´ ì¶”ì¶œ ì‹œë„
            acronym = self._extract_acronym(title)

            # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ (WikiCFP í˜•ì‹ íŒŒì‹± í•„ìš”)
            # ì˜ˆ: "Submission Deadline: Jan 15, 2026"
            submission_deadline = self._extract_date_from_text(description, "submission")

            # ê¸°ë³¸ ì •ë³´ êµ¬ì„±
            conference_data = {
                "conference_name": title,
                "conference_acronym": acronym,
                "website_url": link,
                "submission_deadline": submission_deadline,
                "topics": [category.upper()],
                "tier": self.MAJOR_CONFERENCES.get(acronym, "B") if acronym else "B",
                "is_upcoming": self._is_upcoming(submission_deadline),
            }

            return conference_data

        except Exception as e:
            print(f"âš ï¸ Error parsing WikiCFP entry: {e}")
            return None

    def _extract_acronym(self, title: str) -> Optional[str]:
        """ì œëª©ì—ì„œ ì»¨í¼ëŸ°ìŠ¤ ì•½ì–´ ì¶”ì¶œ"""
        import re

        # ê´„í˜¸ ì•ˆì˜ ì•½ì–´ ì°¾ê¸°: "International Conference on XYZ (ICXYZ) 2026"
        match = re.search(r'\(([A-Z]+)\)', title)
        if match:
            return match.group(1)

        # ì•Œë ¤ì§„ ì»¨í¼ëŸ°ìŠ¤ëª…ì´ ìˆëŠ”ì§€ í™•ì¸
        for conf_name in self.MAJOR_CONFERENCES.keys():
            if conf_name in title.upper():
                return conf_name

        return None

    def _extract_date_from_text(self, text: str, date_type: str = "submission") -> Optional[datetime]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ì¶”ì¶œ

        Args:
            text: íŒŒì‹±í•  í…ìŠ¤íŠ¸
            date_type: ë‚ ì§œ ìœ í˜• (submission, notification, start, end)

        Returns:
            ì¶”ì¶œëœ datetime ê°ì²´
        """
        import re
        from dateutil import parser as date_parser

        try:
            # ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
            patterns = {
                "submission": r"(?:submission|deadline):\s*([A-Za-z]+\s+\d{1,2},?\s+\d{4})",
                "notification": r"notification:\s*([A-Za-z]+\s+\d{1,2},?\s+\d{4})",
                "start": r"(?:start|conference):\s*([A-Za-z]+\s+\d{1,2},?\s+\d{4})",
            }

            pattern = patterns.get(date_type, patterns["submission"])
            match = re.search(pattern, text, re.IGNORECASE)

            if match:
                date_str = match.group(1)
                return date_parser.parse(date_str)

        except Exception as e:
            print(f"âš ï¸ Date parsing error: {e}")

        return None

    def _is_upcoming(self, deadline: Optional[datetime]) -> bool:
        """ë§ˆê°ì¼ì´ ë‹¤ê°€ì˜¤ëŠ” ì»¨í¼ëŸ°ìŠ¤ì¸ì§€ í™•ì¸"""
        if not deadline:
            return False

        now = datetime.now()
        # ë§ˆê°ì¼ì´ í˜„ì¬ë¡œë¶€í„° 6ê°œì›” ì´ë‚´ë©´ upcoming
        return now < deadline < (now + timedelta(days=180))

    async def save_to_db(
        self, conferences: List[Dict[str, Any]], db: AsyncSession
    ) -> int:
        """
        ì»¨í¼ëŸ°ìŠ¤ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥

        Args:
            conferences: ì»¨í¼ëŸ°ìŠ¤ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜

        Returns:
            ì €ì¥ëœ í•­ëª© ìˆ˜
        """
        saved_count = 0

        for conf_data in conferences:
            try:
                # ì¤‘ë³µ í™•ì¸ (URL ê¸°ì¤€)
                website_url = conf_data.get("website_url")
                if not website_url:
                    continue

                result = await db.execute(
                    select(AIConference).where(AIConference.website_url == website_url)
                )
                existing = result.scalar_one_or_none()

                if existing:
                    # ê¸°ì¡´ í•­ëª© ì—…ë°ì´íŠ¸
                    for key, value in conf_data.items():
                        if hasattr(existing, key) and value is not None:
                            setattr(existing, key, value)
                    print(f"ğŸ“ Updated: {conf_data.get('conference_name', 'Unknown')}")
                else:
                    # ìƒˆ í•­ëª© ìƒì„±
                    new_conference = AIConference(**conf_data)
                    db.add(new_conference)
                    saved_count += 1
                    print(f"âœ¨ Created: {conf_data.get('conference_name', 'Unknown')}")

                await db.commit()

            except Exception as e:
                await db.rollback()
                print(f"âŒ Error saving conference: {e}")
                continue

        return saved_count

    async def get_conferences(
        self, db: AsyncSession, skip: int = 0, limit: int = 20, upcoming_only: bool = False
    ) -> List[AIConference]:
        """
        ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì»¨í¼ëŸ°ìŠ¤ ëª©ë¡ ì¡°íšŒ

        Args:
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            skip: ê±´ë„ˆë›¸ í•­ëª© ìˆ˜
            limit: ë°˜í™˜í•  ìµœëŒ€ í•­ëª© ìˆ˜
            upcoming_only: ë‹¤ê°€ì˜¤ëŠ” ì»¨í¼ëŸ°ìŠ¤ë§Œ ì¡°íšŒ

        Returns:
            ì»¨í¼ëŸ°ìŠ¤ ëª©ë¡
        """
        query = select(AIConference)

        if upcoming_only:
            query = query.where(AIConference.is_upcoming == True)

        query = query.order_by(desc(AIConference.submission_deadline)).offset(skip).limit(limit)

        result = await db.execute(query)
        return result.scalars().all()
